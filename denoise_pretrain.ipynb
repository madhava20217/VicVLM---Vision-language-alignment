{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/miniconda3/envs/py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/ml/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmadhava20217\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xa027m34\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tokenizers\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "\n",
    "import wandb \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# from utils.RegVLM import RegVLM\n",
    "from utils.hog import HOGLayer\n",
    "\n",
    "from utils.dataset import pretrain_dataset\n",
    "from utils.mim_utils import create_masked_image\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "\n",
    "DEVICE = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_name = 'ViT-S BERT-S (fixed everything)'\n",
    "algo = 'RegVLM'\n",
    "\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "\n",
    "id = wandb.util.generate_id()\n",
    "wandb.login()\n",
    "\n",
    "# set earlier ID\n",
    "# id = ''\n",
    "\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_JSON = 'Jsons/flickr30k_train.json'\n",
    "MODEL_SAVE_PATH = f'Models/{model_name}/{algo}/checkpoint'\n",
    "\n",
    "lr = 2.5e-4\n",
    "init_lr = 1e-6\n",
    "min_lr = 1e-5\n",
    "decay = 0.01\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "warmup_epochs = 3\n",
    "EPOCHS = 12\n",
    "\n",
    "BATCH_SIZE = 96\n",
    "\n",
    "MASKING_RATIO_IMG = 0.75\n",
    "MASKING_RATIO_TXT = 0.25\n",
    "\n",
    "HOG_BINS = 9\n",
    "\n",
    "ALPHA = 0.995               # EWMA\n",
    "\n",
    "\n",
    "n_layers = 2\n",
    "\n",
    "if os.path.exists(os.path.dirname(MODEL_SAVE_PATH)) == False:\n",
    "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 224\n",
    "\n",
    "MAX_LEN = 30\n",
    "\n",
    "# ViT config\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "## image transforms\n",
    "img_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.int8, scale = True),\n",
    "    v2.Resize(size = (DIMENSION, DIMENSION), antialias = False),\n",
    "    v2.RandAugment(),\n",
    "    # v2.RandomVerticalFlip(),\n",
    "    # v2.RandomHorizontalFlip(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean = [0.5, 0.5, 0.5],\n",
    "        std =  [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pretrain_dataset(\n",
    "               ann_file = [DATASET_JSON],\n",
    "               transform = img_transform,\n",
    "               tokenizer = tokenizer,\n",
    "               max_words = MAX_LEN,\n",
    "               input_size = DIMENSION,\n",
    "               mask_patch_size = 32,\n",
    "               model_patch_size = 16,\n",
    "               masking_ratio = MASKING_RATIO_IMG,\n",
    "               txt_masking_ratio = MASKING_RATIO_TXT,\n",
    "               mask_token = tokenizer.mask_token,\n",
    "               mask_token_id = tokenizer.mask_token_id,\n",
    "               max_length = MAX_LEN + 5,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    pin_memory = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(0, num_layers_to_keep):\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "def deleteLaterEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(num_layers_to_keep, 0, -1):\n",
    "        newModuleList.append(oldModuleList[-i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "class RegVLM_Mixer(torch.nn.Module):\n",
    "    def __init__(self, base_bert,\n",
    "                 n_layers = 2,\n",
    "                 n_visual_tokens = 196,\n",
    "                 vision_embedding_dim = 384,\n",
    "                 emb_dims = 512,\n",
    "                 cls_token_id = 101):\n",
    "        # prepare decoder\n",
    "        super().__init__()\n",
    "        self.cls_token_id = cls_token_id\n",
    "        self.embedding_module = base_bert.embeddings\n",
    "        self.n_visual_tokens = n_visual_tokens\n",
    "        self.vision_emb_dim = vision_embedding_dim\n",
    "        self.base_model = deleteLaterEncodingLayers(base_bert, n_layers).encoder\n",
    "        \n",
    "        self.pooler = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.emb_dimension = emb_dims\n",
    "        \n",
    "        if self.vision_emb_dim == self.emb_dimension:\n",
    "            self.dimension_caster = torch.nn.Identity()\n",
    "        else:\n",
    "            self.dimension_caster = torch.nn.Linear(self.vision_emb_dim, self.emb_dimension, bias = False)  # no bias here\n",
    "        \n",
    "        \n",
    "    def forward(self, vision_embedding, text_embedding, text_attn_mask):\n",
    "        # assert len(vision_embedding) == len(text_embedding)\n",
    "        n_batch = len(vision_embedding)\n",
    "        \n",
    "        cls_emb = self.embedding_module(torch.tensor([[self.cls_token_id]]*n_batch, \n",
    "                                                     device = vision_embedding.device),\n",
    "                                        torch.tensor([[1]]*n_batch,\n",
    "                                                     device = vision_embedding.device))\n",
    "        \n",
    "        # normalize dimensions\n",
    "        new_vision_emb = self.dimension_caster(vision_embedding[:, 1:, :])   # remove cls token here\n",
    "        \n",
    "        # concatenate\n",
    "        concatenated_emb = torch.cat([cls_emb, new_vision_emb, text_embedding], dim = 1)\n",
    "        \n",
    "        # create attention mask\n",
    "        vision_attention_mask = torch.ones(n_batch, self.n_visual_tokens + 1).to(text_attn_mask.device) # add a cls token here\n",
    "        attn_mask = torch.cat([vision_attention_mask, text_attn_mask], dim = 1)\n",
    "        \n",
    "        attn_mask = attn_mask[:, None, None, :]\n",
    "        \n",
    "        # forward\n",
    "        return self.base_model(concatenated_emb, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3336982678.py, line 142)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[56], line 142\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_joint_mlm_loss(self, )\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class RegVLM(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 vit,\n",
    "                 bert,\n",
    "                 vit_num_patches = 196,\n",
    "                 vit_emb_dim = 384,\n",
    "                 bert_emb_dim = 512,\n",
    "                 bert_layers = 2,\n",
    "                 vocab_size = 30522,\n",
    "                 mask_token_id = 103,\n",
    "                 cls_token_id = 101,\n",
    "                 tau = None):\n",
    "       super().__init__()\n",
    "       self.vit = vit.vit\n",
    "       self.mim_reconstruction = vit.decoder\n",
    "       self.bert = bert.base_model\n",
    "       self.bert = deleteEncodingLayers(self.bert.base_model, bert_layers)\n",
    "       self.fusion = RegVLM_Mixer(bert.base_model,\n",
    "                              n_layers = bert_layers,\n",
    "                              n_visual_tokens=vit_num_patches,\n",
    "                              vision_embedding_dim=vit_emb_dim,\n",
    "                              emb_dims = bert_emb_dim,\n",
    "                              cls_token_id = cls_token_id)\n",
    "       \n",
    "       # vit patches data\n",
    "       self.vit_num_patches = vit_num_patches\n",
    "       \n",
    "       # vocab size\n",
    "       self.vocab_size = vocab_size\n",
    "       # mask token\n",
    "       self.mask_token_id = mask_token_id\n",
    "       \n",
    "       # learnable temperature parameter\n",
    "       self.tau = torch.nn.Parameter(torch.FloatTensor([0.07]))      # uniform in range 1 to 5\n",
    "       if tau is not None:\n",
    "           self.tau = torch.nn.Parameter(torch.FloatTensor([tau]))      # uniform in range 1 to 5\n",
    "       \n",
    "       self.tau.requires_grad = True\n",
    "\n",
    "       # joint representation\n",
    "       self.pooler = torch.nn.Sequential(\n",
    "           torch.nn.AdaptiveAvgPool1d(1),\n",
    "           torch.nn.Flatten()\n",
    "       )\n",
    "       self.img_proj = torch.nn.Linear(vit_emb_dim, min(vit_emb_dim, bert_emb_dim))\n",
    "       self.txt_proj = torch.nn.Linear(bert_emb_dim, min(vit_emb_dim, bert_emb_dim))\n",
    "\n",
    "       \n",
    "       # masked representation modeling\n",
    "       self.mrm_proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bert_emb_dim, bert_emb_dim),\n",
    "            torch.nn.Tanh(),\n",
    "       )\n",
    "       \n",
    "       # head for masked image modeling\n",
    "       self.mim_proj = torch.nn.Sequential(\n",
    "           torch.nn.Linear(bert_emb_dim, vit_emb_dim),\n",
    "       )\n",
    "        \n",
    "       # head for masked language modeling\n",
    "       self.mlm_head_joint = copy.deepcopy(bert.cls)\n",
    "       self.mlm_head = bert.cls\n",
    "       \n",
    "       self.itc_head = torch.nn.Linear(bert_emb_dim, bert_emb_dim)\n",
    "       \n",
    "       self.itm__head = torch.nn.Sequential(\n",
    "           torch.nn.Linear(bert_emb_dim, bert_emb_dim),\n",
    "           torch.nn.LeakyReLU(),\n",
    "           torch.nn.Linear(bert_emb_dim, 1)\n",
    "       )\n",
    "    \n",
    "       \n",
    "       \n",
    "    def forward(self, image, text, attn_mask,\n",
    "                masked_pos_img = None,\n",
    "                masked_text = None,\n",
    "                image_text_matching = False,\n",
    "                retrieval = False,\n",
    "                ):\n",
    "        \n",
    "        if retrieval is True:\n",
    "            img_rep = self.vit(image)['last_hidden_state']\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']\n",
    "            joint_rep = self.fusion(img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            \n",
    "            # img_rep = self.img_proj(self.pooler(img_rep.transpose(1,2)))\n",
    "            # txt_rep = self.txt_proj(self.pooler(txt_rep.transpose(1,2)))\n",
    "            return img_rep, txt_rep, joint_rep, self.itm__head(joint_rep[:, 0, :])\n",
    "        \n",
    "        if image_text_matching == True:\n",
    "            img_rep = self.vit(image)['last_hidden_state']\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']\n",
    "            joint_rep = self.fusion(img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            return img_rep, txt_rep, joint_rep, self.itm__head(joint_rep[:, 0, :])\n",
    "        \n",
    "        else:\n",
    "            # return mask_img-clean_txt, clean_img,-mask_txt, \n",
    "            img_rep = self.vit(image)['last_hidden_state']              # clean image\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']   # clean text\n",
    "            \n",
    "            mask_img_rep = self.vit(image, bool_masked_pos = masked_pos_img)['last_hidden_state']\n",
    "            mask_txt_rep = self.bert(masked_text, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            # multimodal prediction\n",
    "            c_img_m_txt = self.fusion(img_rep, mask_txt_rep, attn_mask)['last_hidden_state']\n",
    "            m_img_c_txt = self.fusion(mask_img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            # pure txt\n",
    "            txt_prediction = self.mlm_head(mask_txt_rep)\n",
    "            \n",
    "            # pure fusion\n",
    "            img_txt_joint = self.fusion(img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "        \n",
    "            return (c_img_m_txt, m_img_c_txt, img_txt_joint, mask_img_rep, txt_prediction, img_rep, txt_rep)\n",
    "    \n",
    "    def get_mrm_loss(self, online_representation, target_representation, mask):\n",
    "        # remove cls token\n",
    "        on_rep = torch.nn.functional.normalize(self.mrm_proj(online_representation[:, 1:, :]), dim = -1)\n",
    "        tr_rep = torch.nn.functional.normalize(self.mrm_proj(target_representation[:, 1:, :]), dim = -1)\n",
    "\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(on_rep, tr_rep, reduction = 'none')\n",
    "        mrm_loss = (loss * mask).sum()/(mask.sum() + 1e-5)              # add for 0 division errors\n",
    "        return mrm_loss\n",
    "    \n",
    "    def get_joint_mim_loss(self, joint_rep, img, mask):\n",
    "        joint_rep = joint_rep[:, :self.vit_num_patches+1, :]\n",
    "        joint_rep = self.mim_proj(joint_rep)                    # bs x vit_npatch x vit dim\n",
    "        sequence_output = joint_rep\n",
    "\n",
    "        # Reshape to (batch_size, num_channels, height, width)\n",
    "        sequence_output = sequence_output[:, 1:]\n",
    "        batch_size, sequence_length, num_channels = sequence_output.shape\n",
    "        height = width = math.floor(sequence_length**0.5)\n",
    "        sequence_output = sequence_output.permute(0, 2, 1).reshape(batch_size, num_channels, height, width)\n",
    "\n",
    "        # Reconstruct pixel values\n",
    "        reconstructed_pixel_values = self.decoder(sequence_output)\n",
    "\n",
    "        masked_im_loss = None\n",
    "        if bool_masked_pos is not None:\n",
    "            size = self.config.image_size // self.config.patch_size\n",
    "            bool_masked_pos = bool_masked_pos.reshape(-1, size, size)\n",
    "            mask = (\n",
    "                bool_masked_pos.repeat_interleave(self.config.patch_size, 1)\n",
    "                .repeat_interleave(self.config.patch_size, 2)\n",
    "                .unsqueeze(1)\n",
    "                .contiguous()\n",
    "            )\n",
    "            reconstruction_loss = torch.nn.functional.l1_loss(img, reconstructed_pixel_values, reduction=\"none\")\n",
    "            masked_im_loss = (reconstruction_loss * mask).sum() / (mask.sum() + 1e-5) / 3\n",
    "\n",
    "        return masked_im_loss\n",
    "    \n",
    "    \n",
    "    def get_joint_mlm_loss(self, )\n",
    "    \n",
    "    def get_mlm_loss(self, scores, sen, masked_sen):\n",
    "        labels = torch.where(masked_sen == self.mask_token_id, sen, -100)\n",
    "        loss = torch.nn.functional.cross_entropy(scores.view(-1, self.vocab_size), labels.view(-1), ignore_index=-100)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def get_itc_loss(self, img_feats, txt_feats):\n",
    "        # Calculate similarity\n",
    "        with torch.no_grad():\n",
    "            self.tau.clamp_(0.001,0.5)\n",
    "\n",
    "        # pool and flatten text and visual features obtained before fusion\n",
    "        img_feats = self.img_proj(self.pooler(img_feats.transpose(1,2)))\n",
    "        txt_feats = self.txt_proj(self.pooler(txt_feats.transpose(1,2)))\n",
    "        \n",
    "        \n",
    "        sim = (img_feats@txt_feats.T)/self.tau\n",
    "        # sim = torch.clip(sim, max = 1e4, min = 1e-4)\n",
    "        self_mask = torch.eye(sim.shape[0], device=sim.device)\n",
    "        \n",
    "        loss_i2t = -torch.sum(torch.nn.functional.log_softmax(sim, dim = 1)*self_mask, dim = 1).mean()\n",
    "        loss_t2i = -torch.sum(torch.nn.functional.log_softmax(sim.T, dim = 1)*self_mask, dim = 1).mean()\n",
    "\n",
    "        return sim, (loss_i2t+loss_t2i)/2.0\n",
    "    \n",
    "    def get_samples(self, similarities):\n",
    "        probs = torch.nn.functional.softmax(similarities, dim = 1)\n",
    "        probs = probs.fill_diagonal_(0)         # eliminate full samples\n",
    "        txt_indices = torch.multinomial(probs, num_samples=1, replacement=True).squeeze(1)\n",
    "        img_indices = torch.multinomial(probs.T, num_samples=1, replacement=True).squeeze(1)\n",
    "        \n",
    "        return txt_indices, img_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForMaskedImageModeling were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['decoder.0.bias', 'decoder.0.weight', 'vit.embeddings.mask_token']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vit_model = transformers.ViTForMaskedImageModeling.from_pretrained('WinKawaks/vit-small-patch16-224').to(DEVICE)\n",
    "bert_model = transformers.BertForMaskedLM.from_pretrained(\"prajjwal1/bert-small\")\n",
    "\n",
    "online_network = RegVLM(\n",
    "                    vit = vit_model,\n",
    "                    bert = bert_model,\n",
    "                    vit_num_patches= 196,\n",
    "                    vit_emb_dim=384,\n",
    "                    bert_emb_dim=512,\n",
    "                    bert_layers=n_layers,\n",
    "                    vocab_size=tokenizer.vocab_size,\n",
    "                    mask_token_id= tokenizer.mask_token_id,\n",
    "                    # cls_token_id=tokenizer.cls_token_id\n",
    "                ).train().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in pretrain_dataloader:\n",
    "    img, img_mask, txt, attn_mask, masked_toks, masked_attn_mask, mask_indices = data\n",
    "    img, img_mask, txt, attn_mask, masked_toks, masked_attn_mask, mask_indices = data\n",
    "        \n",
    "    # vision\n",
    "    img = img.to(DEVICE)\n",
    "    img_mask = img_mask.to(DEVICE)\n",
    "        \n",
    "    # language\n",
    "    txt = txt.to(DEVICE)\n",
    "    attn_mask = attn_mask.to(DEVICE)\n",
    "    masked_toks = masked_toks.to(DEVICE)\n",
    "    masked_attn_mask = masked_attn_mask.to(DEVICE)\n",
    "\n",
    "    # indices for masked text: will be used for masked modeling\n",
    "    mask_indices = mask_indices.float().to(DEVICE)\n",
    "    \n",
    "    # # masked image\n",
    "    # masked_image = create_masked_image(img, img_mask)\n",
    "    flattened_img_mask = img_mask.flatten(1).to(DEVICE)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True, False, False],\n",
       "        [ True,  True,  True,  ...,  True, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ..., False,  True,  True],\n",
       "        [False, False, False,  ...,  True, False, False],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_img_mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_transform = HOGLayer(HOG_BINS, pool = 16, groupp).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 197, 512])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # need to do masked image modeling with text supervision \n",
    "    outs = online_network.vit(img, bool_masked_pos =  flattened_img_mask)[0]\n",
    "    txt_rep = online_network.bert(txt, attn_mask)[0]  # clean text\n",
    "    joint = online_network.fusion(outs, txt_rep, attn_mask)['last_hidden_state']\n",
    "\n",
    "joint[:, :online_network.vit_num_patches+1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 197, 384])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 3.3762e+00,  1.1898e+00,  4.9747e+00,  ..., -9.4658e-01,\n",
       "          -6.3289e-01,  9.4220e-01],\n",
       "         [ 9.2341e-02,  1.5397e+00,  1.6549e+00,  ...,  7.8462e-01,\n",
       "          -2.2992e+00,  3.3514e+00],\n",
       "         [-1.7680e-01,  1.8329e+00,  2.6082e+00,  ...,  1.5202e+00,\n",
       "          -2.7215e+00,  4.5280e+00],\n",
       "         ...,\n",
       "         [-1.1351e+00,  9.9755e-01,  4.0025e+00,  ...,  4.1630e-01,\n",
       "          -7.8341e-02,  3.6474e+00],\n",
       "         [-1.7787e+00,  1.5458e+00,  3.9983e+00,  ...,  5.5988e-01,\n",
       "           2.3629e-01,  3.4860e+00],\n",
       "         [-7.9056e-01,  8.4277e-01,  4.3316e+00,  ...,  6.4973e-02,\n",
       "           1.6623e-01,  3.6845e+00]],\n",
       "\n",
       "        [[ 2.2669e-01,  3.1071e-01, -2.0778e-02,  ..., -7.0768e-01,\n",
       "           5.6413e-01,  2.4698e+00],\n",
       "         [-2.5199e+00, -2.2083e+00,  4.4909e-01,  ...,  1.0419e-01,\n",
       "           3.6589e-01,  3.7860e+00],\n",
       "         [ 6.1990e-01, -6.0655e-01,  3.0824e-01,  ...,  3.1788e+00,\n",
       "          -3.6679e-01,  2.2453e+00],\n",
       "         ...,\n",
       "         [ 2.1079e-01, -1.1646e+00,  9.3609e-01,  ...,  1.4880e+00,\n",
       "          -4.3841e-01,  1.8282e+00],\n",
       "         [-1.2671e+00,  9.9709e-01, -8.9495e-01,  ...,  8.4049e-01,\n",
       "           4.0817e-01,  1.9783e+00],\n",
       "         [-2.2324e-02, -2.7367e+00,  1.8613e-01,  ...,  7.1705e-01,\n",
       "          -7.5993e-02,  1.1159e+00]],\n",
       "\n",
       "        [[-3.0893e+00, -4.2729e+00,  2.4993e+00,  ..., -1.5664e+00,\n",
       "           3.7250e+00, -8.9186e-01],\n",
       "         [-1.1277e+00, -2.5611e+00,  2.8784e+00,  ..., -2.0206e-01,\n",
       "          -2.2323e-01,  3.3077e+00],\n",
       "         [-2.8155e+00, -3.4386e+00, -5.5199e-01,  ..., -1.3802e+00,\n",
       "           2.8173e+00,  4.2994e-01],\n",
       "         ...,\n",
       "         [-5.6859e-01, -2.9591e+00,  1.3060e+00,  ..., -2.5038e+00,\n",
       "          -2.2233e+00,  1.9469e+00],\n",
       "         [-1.6837e+00, -3.1117e+00,  1.2278e+00,  ..., -2.9241e+00,\n",
       "          -1.7229e+00,  1.6579e+00],\n",
       "         [-2.6256e+00, -2.6705e+00,  5.6160e-01,  ..., -1.0589e+00,\n",
       "           2.0702e+00,  7.6469e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.7626e+00,  4.3497e-01, -3.2346e+00,  ..., -2.3075e+00,\n",
       "           3.2745e+00, -9.4005e-01],\n",
       "         [-3.9809e+00, -1.8065e-02, -4.7874e+00,  ...,  2.5473e+00,\n",
       "           2.0479e+00, -1.3726e-01],\n",
       "         [ 1.4437e-01,  1.1371e+00,  9.5703e-01,  ..., -9.4614e-04,\n",
       "           6.4255e-01, -3.4425e-01],\n",
       "         ...,\n",
       "         [-3.1564e+00, -2.2835e+00,  5.0711e-01,  ...,  8.4046e-01,\n",
       "          -1.2319e+00, -1.3653e+00],\n",
       "         [-1.8748e+00, -3.0499e+00, -6.3207e-01,  ...,  1.1459e+00,\n",
       "           5.8804e-01, -9.7728e-01],\n",
       "         [-2.0566e+00, -1.0932e+00,  4.2207e-01,  ..., -1.4765e+00,\n",
       "           2.5999e+00, -1.8419e+00]],\n",
       "\n",
       "        [[ 1.9259e+00, -2.5504e+00,  4.7631e+00,  ..., -2.0338e+00,\n",
       "          -1.1216e+00,  8.4292e-01],\n",
       "         [ 1.9639e+00, -2.2526e+00, -9.0056e-01,  ..., -1.9786e+00,\n",
       "           2.0194e+00,  2.8926e+00],\n",
       "         [ 3.0741e-01, -3.8287e-03,  1.3353e+00,  ...,  1.1473e-01,\n",
       "           7.7824e-01,  5.5267e-01],\n",
       "         ...,\n",
       "         [-1.4765e+00, -3.5530e+00,  1.5259e+00,  ..., -1.6832e+00,\n",
       "           1.1903e+00,  2.9671e+00],\n",
       "         [ 4.6121e-01, -3.6262e+00,  1.5705e+00,  ...,  6.1856e-01,\n",
       "           7.4621e-01,  3.8272e+00],\n",
       "         [ 1.3762e+00, -4.5505e+00,  2.2063e+00,  ..., -5.8325e-01,\n",
       "           1.1087e-01, -4.2397e-01]],\n",
       "\n",
       "        [[-6.4620e-01,  4.0977e-01, -4.1227e-01,  ...,  3.0543e+00,\n",
       "          -4.4871e-01,  2.5720e+00],\n",
       "         [-1.2184e+00, -1.4288e+00, -3.1578e+00,  ...,  1.2448e+00,\n",
       "           1.3722e+00,  2.4440e+00],\n",
       "         [-2.7995e+00, -7.1104e-01, -1.3313e+00,  ...,  2.7949e+00,\n",
       "          -1.2575e-01,  2.9352e+00],\n",
       "         ...,\n",
       "         [-2.7928e+00, -1.9336e+00, -6.7366e-01,  ...,  7.8925e-01,\n",
       "          -2.1302e+00,  3.7799e+00],\n",
       "         [-6.1044e-01, -3.7302e-01, -9.3997e-01,  ...,  2.2449e-01,\n",
       "          -7.8495e-01,  2.8935e+00],\n",
       "         [-7.3426e-01, -2.1109e+00,  1.6852e-01,  ..., -7.9090e-02,\n",
       "          -1.6423e+00,  8.9387e-01]]], device='cuda:0'), pooler_output=tensor([[-0.1757,  0.0879, -0.2448,  ...,  0.4485,  0.9111, -0.3298],\n",
       "        [ 0.6642,  0.2669, -0.7173,  ..., -0.9430,  0.7400,  0.1076],\n",
       "        [ 0.8073,  0.1218, -0.7671,  ..., -0.6541,  0.8485,  0.3846],\n",
       "        ...,\n",
       "        [ 0.9324, -0.8822,  0.3714,  ..., -0.7727,  0.4653, -0.3976],\n",
       "        [ 0.7336,  0.7175,  0.5511,  ...,  0.0695,  0.3161, -0.5117],\n",
       "        [-0.5886,  0.8340, -0.8140,  ..., -0.9181,  0.9160,  0.8211]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoglayer = HOGLayer(nbins = HOG_BINS, pool = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for target network\n",
    "\n",
    "# freeze weights\n",
    "def freeze_weights(nw):\n",
    "    for param in nw.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return nw\n",
    "    \n",
    "def ewma_weights(target, current, alpha = 0.995):\n",
    "    sdA = target.state_dict()\n",
    "    sdB = current.state_dict()\n",
    "    \n",
    "    for key in sdA:\n",
    "        sdA[key] = alpha*sdA[key] + (1-alpha)*sdB[key]\n",
    "    \n",
    "    target.load_state_dict(sdA)\n",
    "    return target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = copy.deepcopy(online_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimiser\n",
    "optim = torch.optim.AdamW(online_network.parameters(),\n",
    "                          lr = lr,\n",
    "                          weight_decay = decay,\n",
    "                          betas = [0.9, 0.999],\n",
    "                          )\n",
    "\n",
    "epoch_steps = math.ceil(len(dataset)/BATCH_SIZE)\n",
    "num_steps = int(EPOCHS * epoch_steps)\n",
    "warmup_steps = int(warmup_epochs * epoch_steps)\n",
    "\n",
    "lr_scheduler = CosineLRScheduler(\n",
    "        optim,\n",
    "        t_initial=num_steps,\n",
    "        # t_mul=1.,\n",
    "        lr_min=min_lr,\n",
    "        warmup_lr_init = init_lr,\n",
    "        warmup_t=warmup_steps,\n",
    "        cycle_limit=1,\n",
    "        t_in_epochs=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# # wandB init\n",
    "# wandb.init(\n",
    "#     id = id,# id,\n",
    "#     resume =  'allow',\n",
    "#     project = 'MAMO - Pretrain',\n",
    "#     name = 'MAMO - ViT-S, BERT-S',\n",
    "\n",
    "#     config = {\n",
    "#         'architecture': model_name,\n",
    "#         'dataset':'ImageNet1K',\n",
    "#         'warmup_epochs': warmup_epochs,\n",
    "#         'epochs' : EPOCHS,\n",
    "#         'batch_size': BATCH_SIZE,\n",
    "#         'masking_ratio_img' : MASKING_RATIO_IMG,\n",
    "#         'masking_ratio_itxt' : MASKING_RATIO_TXT,\n",
    "#         'mask_patch_size': 196,\n",
    "#         'image_size' : DIMENSION,\n",
    "#         'optim_params':{\n",
    "#             'optim': 'AdamW',\n",
    "#             'beta1': beta1,\n",
    "#             'beta2': beta2,\n",
    "#             'weight_decay': decay,\n",
    "#             'learning_rate': lr,\n",
    "#         },\n",
    "#         'accumulation_iters': 1,\n",
    "#         'patch_size_mask' : 32,\n",
    "#         'alpha_ewma': ALPHA,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nums = [re.match(r'.*checkpoint_(.*).pth', x).group(1) for x in glob.glob(MODEL_SAVE_PATH+'*.pth')]\n",
    "# if len(nums) > 0:\n",
    "#     nums.remove(\"final\")\n",
    "nums = [int(x) for x in nums]\n",
    "\n",
    "CHKPT = -1\n",
    "\n",
    "if len(nums) != 0:\n",
    "    CHKPT = max(nums)\n",
    "\n",
    "    load_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, CHKPT)\n",
    "    chkpt = torch.load(load_path, map_location = {'cuda:1': device, \n",
    "                                                  'cuda:0': device})\n",
    "\n",
    "    online_network.load_state_dict(chkpt['online_model_state_dict'])\n",
    "    target_network.load_state_dict(chkpt['target_model_state_dict'])\n",
    "    optim.load_state_dict(chkpt['optim_state_dict'])\n",
    "    # lr_scheduler.load_state_dict(chkpt['scheduler_state_dict'])\n",
    "    \n",
    "    print(load_path)\n",
    "    \n",
    "    print(\"loaded earlier settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = freeze_weights(target_network).to(DEVICE).eval()\n",
    "scaler = torch.cuda.amp.grad_scaler.GradScaler()\n",
    "itm_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(CHKPT+1, EPOCHS + warmup_epochs):\n",
    "    num_samples = 0\n",
    "    pretrain_loss = 0\n",
    "    # net all losses\n",
    "    net_mrm_loss = 0\n",
    "    net_mim_loss = 0\n",
    "    net_mlm_loss = 0\n",
    "    net_itc_loss = 0\n",
    "    net_itm_loss = 0\n",
    "    for idx, data in (pbar := tqdm(enumerate(pretrain_dataloader), total = len(pretrain_dataloader))):\n",
    "        img, img_mask, txt, attn_mask, masked_toks, masked_attn_mask, mask_indices = data\n",
    "        \n",
    "        # vision\n",
    "        img = img.to(DEVICE)\n",
    "        img_mask = img_mask.to(DEVICE)\n",
    "        \n",
    "        # language\n",
    "        txt = txt.to(DEVICE)\n",
    "        attn_mask = attn_mask.to(DEVICE)\n",
    "        masked_toks = masked_toks.to(DEVICE)\n",
    "        masked_attn_mask = masked_attn_mask.to(DEVICE)\n",
    "\n",
    "        # indices for masked text: will be used for masked modeling\n",
    "        mask_indices = mask_indices.float().to(DEVICE)\n",
    "        \n",
    "        # # masked image\n",
    "        # masked_image = create_masked_image(img, img_mask)\n",
    "        flattened_img_mask = img_mask.float().flatten(1)\n",
    "        \n",
    "        # create masks for joint representation modeling\n",
    "        img_rep_masks = torch.cat([flattened_img_mask, torch.zeros_like(mask_indices)], axis = 1).unsqueeze(-1)\n",
    "        txt_rep_masks = torch.cat([torch.zeros_like(flattened_img_mask), mask_indices], axis = 1).unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # masked modeling pretraining\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):                           # casting to f16\n",
    "            \n",
    "            \n",
    "            # forward step for target network\n",
    "            with torch.no_grad():\n",
    "                target_img_rep, target_txt_rep, target_mm_rep, target_itm = target_network(img,\n",
    "                                                                                           txt,\n",
    "                                                                                           attn_mask,\n",
    "                                                                                           image_text_matching = True)\n",
    "            \n",
    "            # forward step for online network\n",
    "            c_img_m_txt, m_img_c_txt, img_txt_joint, mask_img_rep, txt_prediction, img_rep, txt_rep = online_network(img,\n",
    "                                                                                                      txt,\n",
    "                                                                                                      attn_mask,\n",
    "                                                                                                      image_text_matching = False,\n",
    "                                                                                                      masked_pos = mask_indices,\n",
    "                                                                                                      masked_text = masked_toks)\n",
    "\n",
    "\n",
    "            # MRM loss\n",
    "            mrm_loss_txt = online_network.get_mrm_loss(c_img_m_txt, target_mm_rep, txt_rep_masks)\n",
    "            mrm_loss_img = online_network.get_mrm_loss(m_img_c_txt, target_mm_rep, img_rep_masks)\n",
    "            \n",
    "            # MIM loss\n",
    "            mim_loss = online_network.get_mim_loss(m_img_c_txt, target_img_rep, flattened_img_mask)\n",
    "            \n",
    "            # MLM loss\n",
    "            mlm_loss = online_network.get_mlm_loss(txt_prediction, txt, masked_toks)\n",
    "            \n",
    "            \n",
    "            # ITC loss\n",
    "            sim, itc_loss = online_network.get_itc_loss(img_rep, txt_rep)\n",
    "            \n",
    "            #itm loss\n",
    "            # sample for each image and each text separately\n",
    "            neg_txt, neg_img = online_network.get_samples(sim)\n",
    "            \n",
    "            itm_labels = torch.cat([torch.ones(len(img)),torch.zeros(2*len(img))],\n",
    "                               dim=0).unsqueeze(1).float().to(DEVICE)\n",
    "            # stack \n",
    "            itm_img_feats = torch.vstack([img_rep, img_rep[neg_img]])\n",
    "            itm_txt_feats = torch.vstack([txt_rep[neg_txt], txt_rep])\n",
    "            itm_txt_attn = torch.vstack([attn_mask[neg_txt], attn_mask])\n",
    "\n",
    "            joint_rep_negs = online_network.mamo(itm_img_feats, itm_txt_feats, itm_txt_attn)['last_hidden_state']\n",
    "            combined_mamo_reps = torch.vstack([img_txt_joint, joint_rep_negs])\n",
    "            \n",
    "            itm_outputs = online_network.itm__head(combined_mamo_reps[:, 0, :])\n",
    "            \n",
    "            \n",
    "            # softmax probabilities\n",
    "            itm_loss = itm_loss_fn(itm_outputs, itm_labels)\n",
    "            \n",
    "            # TOTAL LOSS\n",
    "            net_loss = (mrm_loss_img + mrm_loss_txt) + (mim_loss) + (mlm_loss) + (itc_loss) + (itm_loss)\n",
    "            \n",
    "        scaler.scale(net_loss).backward()\n",
    "        scaler.unscale_(optim)\n",
    "        torch.nn.utils.clip_grad_norm_(online_network.parameters(), 1.)\n",
    "            \n",
    "        # BACKPROP\n",
    "        scaler.step(optim)        # fp16\n",
    "        scaler.update()           # fp16\n",
    "        optim.zero_grad(set_to_none = True)\n",
    "        lr_scheduler.step_update(epoch * epoch_steps + idx)\n",
    "        \n",
    "        # update and calc loss\n",
    "        num_samples+=1\n",
    "        net_mrm_loss+= mrm_loss_img.item() + mrm_loss_txt.item()\n",
    "        net_mim_loss+= mim_loss.item()\n",
    "        net_mlm_loss+= mlm_loss.item()\n",
    "        net_itc_loss+= itc_loss.item()\n",
    "        net_itm_loss+= itm_loss.item()\n",
    "        pretrain_loss+= net_loss.item()\n",
    "        pbar.set_description(f\"Train Loss: {pretrain_loss/num_samples}\")\n",
    "        \n",
    "        \n",
    "        # EWMA for weights\n",
    "        target_network = ewma_weights(target_network, online_network, alpha = ALPHA)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'pretrain_loss': pretrain_loss/num_samples,\n",
    "            'mrm_loss': net_mrm_loss/num_samples,\n",
    "            'mim_loss': net_mim_loss/num_samples,\n",
    "            'mlm_loss': net_mlm_loss/num_samples,\n",
    "            'itc_loss': net_itc_loss/num_samples,\n",
    "            'itm_loss': net_itm_loss/num_samples,\n",
    "    })\n",
    "    save_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, epoch)\n",
    "    torch.save(\n",
    "            {\n",
    "            'epoch': epoch,\n",
    "            'online_model_state_dict': online_network.state_dict(),\n",
    "            'target_model_state_dict': target_network.state_dict(),\n",
    "            'optim_state_dict': optim.state_dict()\n",
    "            },\n",
    "        save_path\n",
    "        )\n",
    "    if (epoch-warmup_epochs+1) % 10 == 0:\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, 'final')\n",
    "torch.save(online_network.state_dict(), save_path)\n",
    "wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
