{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/miniconda3/envs/py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/ml/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmadhava20217\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yod1ogj5\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tokenizers\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "\n",
    "import wandb \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "\n",
    "DEVICE = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_name = 'vit_bert_s - randn temperature'\n",
    "algo = 'MAMO'\n",
    "\n",
    "\n",
    "NUM_WORKERS = 14\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "\n",
    "id = wandb.util.generate_id()\n",
    "wandb.login()\n",
    "\n",
    "# set earlier ID\n",
    "# id = '8z7zcst9'\n",
    "id = 'yod1ogj5'\n",
    "\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SRC = '../Datasets/Flickr30k/'\n",
    "MODEL_SAVE_PATH = f'Models/{model_name}/{algo}/checkpoint'\n",
    "\n",
    "VOCAB_PATH = 'Vocabulary/flickr30k.vocab'\n",
    "\n",
    "lr = 1e-4\n",
    "init_lr = 1e-6\n",
    "min_lr = 1e-5\n",
    "decay = 0.01\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "warmup_epochs = 5\n",
    "EPOCHS = 40\n",
    "\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "MASKING_RATIO_IMG = 0.75\n",
    "MASKING_RATIO_TXT = 0.25\n",
    "\n",
    "ALPHA = 0.995               # EWMA\n",
    "\n",
    "\n",
    "n_layers = 3\n",
    "\n",
    "if os.path.exists(os.path.dirname(MODEL_SAVE_PATH)) == False:\n",
    "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH))\n",
    "    \n",
    "weights_path = 'Models/vit_bert_s - randn temperature/MAMO/checkpoint_44.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(0, num_layers_to_keep):\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "def deleteLaterEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(num_layers_to_keep, 0, -1):\n",
    "        newModuleList.append(oldModuleList[-i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "\n",
    "def get_bert_model(model, num_layers):\n",
    "    return deleteEncodingLayers(model, num_layers)\n",
    "\n",
    "\n",
    "\n",
    "class MAMO_mixer(torch.nn.Module):\n",
    "    def __init__(self, base_bert, n_layers = 2, n_visual_tokens = 197, vision_embedding_dim = 384, emb_dims = 512):\n",
    "        # prepare decoder\n",
    "        super().__init__()\n",
    "        self.n_visual_tokens = n_visual_tokens\n",
    "        self.vision_emb_dim = vision_embedding_dim\n",
    "        self.base_model = deleteLaterEncodingLayers(base_bert.base_model, n_layers).encoder\n",
    "        \n",
    "        self.pooler = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.emb_dimension = emb_dims\n",
    "        \n",
    "        if self.vision_emb_dim == self.emb_dimension:\n",
    "            self.dimension_caster = torch.nn.Identity()\n",
    "        else:\n",
    "            self.dimension_caster = torch.nn.Linear(self.vision_emb_dim, self.emb_dimension, bias = False)  # no bias here\n",
    "        \n",
    "        \n",
    "    def forward(self, vision_embedding, text_embedding, text_attn_mask):\n",
    "        # assert len(vision_embedding) == len(text_embedding)\n",
    "        n_batch = len(vision_embedding)\n",
    "        \n",
    "        # normalize dimensions\n",
    "        new_vision_emb = self.dimension_caster(vision_embedding)\n",
    "        \n",
    "        # concatenate\n",
    "        concatenated_emb = torch.cat([new_vision_emb, text_embedding], dim = 1)\n",
    "        \n",
    "        # create attention mask\n",
    "        vision_attention_mask = torch.ones(n_batch, self.n_visual_tokens).to(text_attn_mask.device)\n",
    "        attn_mask = torch.cat([vision_attention_mask, text_attn_mask], dim = 1)\n",
    "        \n",
    "        attn_mask = attn_mask[:, None, None, :]\n",
    "        \n",
    "        # forward\n",
    "        return self.base_model(concatenated_emb, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMO(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 vit,\n",
    "                 bert,\n",
    "                 vit_num_patches = 196,\n",
    "                 vit_emb_dim = 384,\n",
    "                 bert_emb_dim = 512,\n",
    "                 bert_layers = 2,\n",
    "                 vocab_size = 30522,\n",
    "                 mask_token_id = 103):\n",
    "       super().__init__()\n",
    "       self.vit = vit\n",
    "       self.bert = bert.base_model\n",
    "       self.bert = deleteEncodingLayers(self.bert.base_model, bert_layers)\n",
    "       self.mamo = MAMO_mixer(bert, bert_layers, 197, vit_emb_dim)\n",
    "       \n",
    "       # vit patches data\n",
    "       self.vit_num_patches = vit_num_patches\n",
    "       \n",
    "       # vocab size\n",
    "       self.vocab_size = vocab_size\n",
    "       # mask token\n",
    "       self.mask_token_id = mask_token_id\n",
    "       \n",
    "       # learnable temperature parameter\n",
    "       self.tau = torch.nn.Parameter(torch.Tensor([1.]))#torch.FloatTensor(1).uniform_(2, 5))       # uniform in range 1 to 5\n",
    "       self.tau.requires_grad = True\n",
    "\n",
    "       # joint representation\n",
    "       self.pooler = torch.nn.Sequential(\n",
    "           torch.nn.AdaptiveAvgPool1d(1),\n",
    "           torch.nn.Flatten()\n",
    "       )\n",
    "       self.img_proj = torch.nn.Linear(vit_emb_dim, min(vit_emb_dim, bert_emb_dim))\n",
    "       self.txt_proj = torch.nn.Linear(bert_emb_dim, min(vit_emb_dim, bert_emb_dim))\n",
    "\n",
    "       \n",
    "       # masked representation modeling\n",
    "       self.mrm_proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bert_emb_dim, bert_emb_dim),\n",
    "            torch.nn.Tanh(),\n",
    "       )\n",
    "       \n",
    "       # head for masked image modeling\n",
    "       self.mim_proj = torch.nn.Sequential(\n",
    "           torch.nn.Linear(bert_emb_dim, vit_emb_dim),\n",
    "       )\n",
    "        \n",
    "       # head for masked language modeling\n",
    "       self.mlm_head = bert.cls\n",
    "       \n",
    "       self.itc_head = torch.nn.Linear(bert_emb_dim, bert_emb_dim)\n",
    "       \n",
    "       self.itm__head = torch.nn.Sequential(\n",
    "           torch.nn.Linear(bert_emb_dim, bert_emb_dim),\n",
    "           torch.nn.LeakyReLU(),\n",
    "           torch.nn.Linear(bert_emb_dim, 1)\n",
    "       )\n",
    "    \n",
    "       \n",
    "       \n",
    "    def forward(self, image, text, attn_mask,\n",
    "                masked_image = None,\n",
    "                masked_text = None,\n",
    "                image_text_matching = False,\n",
    "                ):\n",
    "        \n",
    "        if image_text_matching == True:\n",
    "            img_rep = torch.nn.functional.normalize(self.vit(image)['last_hidden_state'], p = 2, dim = 2)\n",
    "            txt_rep = torch.nn.functional.normalize(self.bert(text, attn_mask)['last_hidden_state'], p = 2, dim = 2)\n",
    "            joint_rep = self.mamo(img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            return img_rep, txt_rep, joint_rep, self.itm__head(joint_rep)[:, 0, :]\n",
    "        \n",
    "        else:\n",
    "            # return mask_img-clean_txt, clean_img,-mask_txt, \n",
    "            img_rep = self.vit(image)['last_hidden_state']              # clean image\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']   # clean text\n",
    "            \n",
    "            mask_img_rep = self.vit(masked_image)['last_hidden_state']\n",
    "            mask_txt_rep = self.bert(masked_text, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            # multimodal prediction\n",
    "            c_img_m_txt = self.mamo(img_rep, mask_txt_rep, attn_mask)['last_hidden_state']\n",
    "            m_img_c_txt = self.mamo(mask_img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            # pure txt\n",
    "            txt_prediction = self.mlm_head(mask_txt_rep)\n",
    "            \n",
    "            # pool and flatten text and visual features obtained before fusion\n",
    "            img_rep = torch.nn.functional.normalize(self.img_proj(self.pooler(img_rep.transpose(1,2))), 2)\n",
    "            txt_rep = torch.nn.functional.normalize(self.txt_proj(self.pooler(txt_rep.transpose(1,2))), 2)\n",
    "            \n",
    "            return (c_img_m_txt, m_img_c_txt, mask_img_rep, txt_prediction, img_rep, txt_rep)\n",
    "            \n",
    "    def mrm_projection(self, rep):\n",
    "        return torch.nn.functional.normalize(self.mrm_proj(rep), dim = 2)\n",
    "    \n",
    "    def mim_projection(self, rep):\n",
    "        return torch.nn.functional.normalize(self.mim_proj(rep), dim = 2)\n",
    "    \n",
    "    def get_mrm_loss(self, online_representation, target_representation, mask):\n",
    "        # remove cls token\n",
    "        on_rep = self.mrm_projection(online_representation[:, 1:, :])\n",
    "        tr_rep = target_representation[:, 1:, :]\n",
    "        \n",
    "        # normalize\n",
    "        on_rep = torch.nn.functional.normalize(on_rep, dim = 2)\n",
    "        tr_rep = torch.nn.functional.normalize(tr_rep, dim = 2)\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(on_rep, tr_rep, reduction = 'none')\n",
    "        mrm_loss = (loss * mask).sum()/(mask.sum() + 1e-5)              # add for 0 division errors\n",
    "        return mrm_loss\n",
    "    \n",
    "    def get_mim_loss(self, online_representation, target_representation, mask):\n",
    "        on_rep = self.mim_projection(online_representation[:, 1:self.vit_num_patches+1, :]) # omit cls token\n",
    "        tr_rep = target_representation[:, 1:self.vit_num_patches+1, :]\n",
    "        \n",
    "        # normalize\n",
    "        on_rep = torch.nn.functional.normalize(on_rep, dim = 2)\n",
    "        tr_rep = torch.nn.functional.normalize(tr_rep, dim = 2)\n",
    "        \n",
    "        loss = torch.nn.functional.l1_loss(on_rep, tr_rep, reduction = 'none')\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask[:, :, None]\n",
    "        mim_loss = (loss * mask).sum() / (mask.sum() + 1e-5)\n",
    "        return mim_loss \n",
    "    \n",
    "    \n",
    "    def get_mlm_loss(self, scores, sen, masked_sen):\n",
    "        labels = torch.where(masked_sen == self.mask_token_id, sen, -100)\n",
    "        loss = torch.nn.functional.cross_entropy(scores.view(-1, self.vocab_size), labels.view(-1), ignore_index=-100)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def get_itc_loss(self, img_feats, txt_feats):\n",
    "        # Calculate cosine similarity\n",
    "        sim = torch.exp((img_feats@txt_feats.T)/self.tau)\n",
    "        self_mask = torch.eye(sim.shape[0], device=sim.device)\n",
    "\n",
    "        return sim, (torch.nn.functional.cross_entropy(sim, self_mask) + torch.nn.functional.cross_entropy(sim.T, self_mask))/2.\n",
    "    \n",
    "    def get_samples(self, similarities):\n",
    "        probs = torch.nn.functional.softmax(similarities, dim = 1)\n",
    "        txt_indices = torch.multinomial(probs, num_samples=1, replacement=True).squeeze(1)\n",
    "        img_indices = torch.multinomial(probs.T, num_samples=1, replacement=True).squeeze(1)\n",
    "        \n",
    "        return txt_indices, img_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DIMENSION = 224\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "# ViT config\n",
    "feature_extractor = transformers.AutoFeatureExtractor.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
    "\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "## image transforms\n",
    "img_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.int8, scale = True),\n",
    "    v2.Resize(size = (DIMENSION, DIMENSION), antialias = False),\n",
    "    v2.RandAugment(),\n",
    "    # v2.RandomVerticalFlip(),\n",
    "    # v2.RandomHorizontalFlip(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean = [0.5, 0.5, 0.5],\n",
    "        std =  [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_model = transformers.ViTModel.from_pretrained('WinKawaks/vit-small-patch16-224').to(DEVICE)\n",
    "bert_model = transformers.BertForMaskedLM.from_pretrained(\"prajjwal1/bert-small\")\n",
    "\n",
    "online_network = MAMO(\n",
    "                    vit = vit_model,\n",
    "                    bert = bert_model,\n",
    "                    vit_num_patches= 196,\n",
    "                    vit_emb_dim=384,\n",
    "                    bert_emb_dim=512,\n",
    "                    bert_layers=n_layers,\n",
    "                    vocab_size=tokenizer.vocab_size,\n",
    "                    mask_token_id= tokenizer.mask_token_id\n",
    "                ).to(DEVICE)\n",
    "\n",
    "\n",
    "chkpt = torch.load(weights_path, map_location=device)['online_model_state_dict']\n",
    "online_network.load_state_dict(chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30K_Finetune(torchvision.datasets.Flickr30k):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 ann_path,\n",
    "                 img_transform = None,\n",
    "                 txt_transform = None,\n",
    "                 max_length = 50,\n",
    "                 ):\n",
    "        super().__init__(data_path, ann_path)\n",
    "        self.img_transform = img_transform\n",
    "        self.tokenizer = txt_transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def process_string(self, txts):\n",
    "        ret = []\n",
    "        for string in txts:\n",
    "            tok_str = string.lower().split() # separated by spaces\n",
    "            stopwords = nltk.corpus.stopwords.words('english')\n",
    "            proc_str = [x for x in tok_str if x not in stopwords]               # stopword removal\n",
    "            proc_str = [word.lower() for word in proc_str if word.isalpha()]    # punctuation removal\n",
    "            ret.append(\" \".join(proc_str))\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, txt = super().__getitem__(idx)\n",
    "        # get images and texts\n",
    "        img = self.img_transform(img)\n",
    "        \n",
    "        # process string\n",
    "        txt = self.process_string(txt)\n",
    "        tok_text = self.tokenizer(txt, truncation = True, padding = 'max_length', max_length = self.max_length, return_token_type_ids=False)\n",
    "        toks, attn_mask = tok_text['input_ids'], tok_text['attention_mask']        \n",
    "        toks, attn_mask = torch.tensor(toks), torch.tensor(attn_mask)\n",
    "        \n",
    "        return img, toks, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Flickr30K_Finetune(\n",
    "    DATASET_SRC + 'flickr30k-images',\n",
    "    DATASET_SRC + 'results_20130124.token',\n",
    "    img_transform,\n",
    "    tokenizer,\n",
    "    MAX_LEN\n",
    ")\n",
    "\n",
    "# dataloader\n",
    "\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    pin_memory = True,\n",
    "    num_workers = 0,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "\n",
    "for idx, data in enumerate(pretrain_dataloader):\n",
    "    img, txt, attn_mask = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 5, 50])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimiser\n",
    "optim = torch.optim.AdamW(online_network.parameters(),\n",
    "                          lr = lr,\n",
    "                          weight_decay = decay,\n",
    "                          betas = [0.9, 0.999],\n",
    "                          )\n",
    "\n",
    "epoch_steps = math.ceil(len(dataset)/BATCH_SIZE)\n",
    "num_steps = int(EPOCHS * epoch_steps)\n",
    "warmup_steps = int(warmup_epochs * epoch_steps)\n",
    "\n",
    "lr_scheduler = CosineLRScheduler(\n",
    "        optim,\n",
    "        t_initial=num_steps,\n",
    "        # t_mul=1.,\n",
    "        lr_min=min_lr,\n",
    "        warmup_lr_init = init_lr,\n",
    "        warmup_t=warmup_steps,\n",
    "        cycle_limit=1,\n",
    "        t_in_epochs=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
