{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H1r7bAUV0X-7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ml/miniconda3/envs/py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package stopwords to /home/ml/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmadhava20217\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'gk827qfr'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torchinfo import summary\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import tokenizers\n",
        "import itertools\n",
        "\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "\n",
        "from utils.MAMO import MAMO\n",
        "from utils.dataset import re_train_dataset, re_eval_dataset\n",
        "\n",
        "import wandb\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "device = 'cuda:1'\n",
        "\n",
        "DEVICE = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
        "model_name = 'vit_bert_s - normalized'\n",
        "algo = 'MAMO'\n",
        "\n",
        "# fix the seed for reproducibility\n",
        "seed = 6969\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "id = wandb.util.generate_id()\n",
        "wandb.login()\n",
        "\n",
        "NUM_WORKERS = 8\n",
        "torch.set_num_threads(12)\n",
        "\n",
        "id = 'gk827qfr'\n",
        "id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3YBytuU6NIii"
      },
      "outputs": [],
      "source": [
        "# assert False, \"weights_path\"\n",
        "weights_path = 'Models/vit_bert_s - normalized/MAMO/checkpoint_19.pth'\n",
        "\n",
        "MODEL_SAVE_PATH = MODEL_SAVE_PATH = f'Finetuning/{model_name}/{algo}/checkpoint'\n",
        "if os.path.exists(os.path.dirname(MODEL_SAVE_PATH)) == False:\n",
        "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wii4D6Dnwmey"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader, tokenizer, device, k=10, max_len = 30):\n",
        "    # test; k for top-k; tokenizer is model.bert\n",
        "    model.eval()\n",
        "\n",
        "    texts = data_loader.dataset.text\n",
        "    num_text = len(texts)\n",
        "    text_bs = 64\n",
        "    text_feats = []\n",
        "    text_embeds = []\n",
        "    text_atts = []\n",
        "    for i in range(0, num_text, text_bs):\n",
        "        text = texts[i: min(num_text, i+text_bs)]\n",
        "        text_input = tokenizer(text, padding='max_length', truncation=True, max_length=max_len + 5, return_tensors=\"pt\").to(device)\n",
        "        text_output = model.bert(text_input.input_ids, text_input.attention_mask)\n",
        "        text_feat = torch.nn.functional.normalize(text_output['last_hidden_state'])\n",
        "        text_embed = model.text_proj(model.pooler(text_feat.transpose(1,2)), dim = 2)\n",
        "        text_embeds.append(text_embed)\n",
        "        text_feats.append(text_feat)\n",
        "        text_atts.append(text_input.attention_mask)\n",
        "    text_embeds = torch.cat(text_embeds,dim=0)\n",
        "    text_feats = torch.cat(text_feats,dim=0)\n",
        "    text_atts = torch.cat(text_atts,dim=0)\n",
        "\n",
        "    image_feats = []\n",
        "    image_embeds = []\n",
        "    for image, img_id in data_loader:\n",
        "        image = image.to(device)\n",
        "        image_feat = torch.nn.functional.normalize(model.vit(image)['last_hidden_state'])\n",
        "        image_embed = model.img_proj(model.pooler(image_feat.transpose(1, 2)), dim = 2)\n",
        "\n",
        "        image_feats.append(image_feat)\n",
        "        image_embeds.append(image_embed)\n",
        "\n",
        "    image_feats = torch.cat(image_feats,dim=0)\n",
        "    image_embeds = torch.cat(image_embeds,dim=0)\n",
        "\n",
        "    sims_matrix = image_embeds @ text_embeds.t()\n",
        "    score_matrix_i2t = torch.full((len(data_loader.dataset.image),len(texts)),-100.0).to(device)\n",
        "\n",
        "\n",
        "    for i,sims in enumerate(sims_matrix):\n",
        "        topk_sim, topk_idx = sims.topk(k=k, dim=0)\n",
        "\n",
        "        encoder_output = image_feats[i].repeat(k,1,1)\n",
        "        # encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
        "        output = model.mamo(encoder_output,\n",
        "                            text_feats[topk_idx],\n",
        "                            text_atts[topk_idx])['last_hidden_state']\n",
        "        \n",
        "        score = 1 - model.itm__head(output.last_hidden_state[:,0,:])#[:,1]          # take output for prediction head 1\n",
        "        score_matrix_i2t[i,topk_idx] = score\n",
        "\n",
        "    sims_matrix = sims_matrix.t()\n",
        "    score_matrix_t2i = torch.full((len(texts),len(data_loader.dataset.image)),-100.0).to(device)\n",
        "\n",
        "    for i,sims in enumerate(sims_matrix):\n",
        "        topk_sim, topk_idx = sims.topk(k=k, dim=0)\n",
        "        encoder_output = image_feats[topk_idx]\n",
        "        # encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
        "        output = model.mamo(encoder_output,\n",
        "                                    text_feats[i].repeat(k,1,1),\n",
        "                                    text_atts[i].repeat(k,1))['last_hidden_state']\n",
        "        score = 1 - model.itm__head(output.last_hidden_state[:,0,:])#[:,1]\n",
        "        score_matrix_t2i[i,topk_idx] = score\n",
        "\n",
        "    return score_matrix_i2t.cpu().numpy(), score_matrix_t2i.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def itm_eval(scores_i2t, scores_t2i, txt2img, img2txt):\n",
        "\n",
        "    #Images->Text\n",
        "    ranks = np.zeros(scores_i2t.shape[0])\n",
        "    for index,score in enumerate(scores_i2t):\n",
        "        inds = np.argsort(score)[::-1]\n",
        "        # Score\n",
        "        rank = 1e20\n",
        "        for i in img2txt[index]:\n",
        "            tmp = np.where(inds == i)[0][0]\n",
        "            if tmp < rank:\n",
        "                rank = tmp\n",
        "        ranks[index] = rank\n",
        "\n",
        "    # Compute metrics\n",
        "    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
        "    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
        "    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
        "\n",
        "    #Text->Images\n",
        "    ranks = np.zeros(scores_t2i.shape[0])\n",
        "\n",
        "    for index,score in enumerate(scores_t2i):\n",
        "        inds = np.argsort(score)[::-1]\n",
        "        ranks[index] = np.where(inds == txt2img[index])[0][0]\n",
        "\n",
        "    # Compute metrics\n",
        "    ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
        "    ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
        "    ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
        "\n",
        "    tr_mean = (tr1 + tr5 + tr10) / 3\n",
        "    ir_mean = (ir1 + ir5 + ir10) / 3\n",
        "    r_mean = (tr_mean + ir_mean) / 2\n",
        "\n",
        "    eval_result =  {'txt_r1': tr1,\n",
        "                    'txt_r5': tr5,\n",
        "                    'txt_r10': tr10,\n",
        "                    'txt_r_mean': tr_mean,\n",
        "                    'img_r1': ir1,\n",
        "                    'img_r5': ir5,\n",
        "                    'img_r10': ir10,\n",
        "                    'img_r_mean': ir_mean,\n",
        "                    'r_mean': r_mean}\n",
        "    return eval_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aQhQuMVd04K8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import v2\n",
        "from PIL import Image\n",
        "\n",
        "def create_dataset(dataset, config):\n",
        "\n",
        "    normalize = v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "    train_transform = v2.Compose([\n",
        "            v2.ToDtype(torch.int8, scale = True),\n",
        "            v2.RandomResizedCrop(config['image_res'],scale=(0.5, 1.0), interpolation=Image.BICUBIC),\n",
        "            v2.RandomHorizontalFlip(),\n",
        "            v2.RandAugment(2,7),\n",
        "            v2.ToImage(),\n",
        "            v2.ToDtype(torch.float32, scale = True),\n",
        "            normalize,\n",
        "        ])\n",
        "    test_transform = v2.Compose([\n",
        "        v2.Resize((config['image_res'],config['image_res']),interpolation=Image.BICUBIC),\n",
        "        v2.ToTensor(),\n",
        "        normalize,\n",
        "        ])\n",
        "\n",
        "    if dataset=='re':\n",
        "        train_dataset = re_train_dataset(config['train_file'], train_transform, config['image_root'])\n",
        "        val_dataset = re_eval_dataset(config['val_file'], test_transform, config['image_root'])\n",
        "        test_dataset = re_eval_dataset(config['test_file'], test_transform, config['image_root'])\n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def create_loader(datasets, samplers, batch_size, num_workers, is_trains, collate_fns):\n",
        "    loaders = []\n",
        "    for dataset,sampler,bs,n_worker,is_train,collate_fn in zip(datasets,samplers,batch_size,num_workers,is_trains,collate_fns):\n",
        "        if is_train:\n",
        "            shuffle = (sampler is None)\n",
        "            drop_last = True\n",
        "        else:\n",
        "            shuffle = False\n",
        "            drop_last = False\n",
        "        loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=bs,\n",
        "            num_workers=n_worker,\n",
        "            pin_memory=True,\n",
        "            sampler=sampler,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=collate_fn,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        loaders.append(loader)\n",
        "    return loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_LENGTH = 30\n",
        "BATCH_SIZE = 16\n",
        "DIMENSION = 224\n",
        "EPOCHS = 30\n",
        "warmup_epochs = 5\n",
        "\n",
        "lr = 2.5e-4\n",
        "init_lr = 1e-6\n",
        "min_lr = 1e-5\n",
        "decay = 0.01\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "\n",
        "n_layers = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ml/miniconda3/envs/py3.11/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "config = {'train_file': ['Jsons/flickr30k_train.json'],\n",
        "          'val_file': 'Jsons/flickr30k_val.json',\n",
        "          'test_file': 'Jsons/flickr30k_test.json',\n",
        "          'image_root': './',\n",
        "          'image_res': DIMENSION}\n",
        "\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_dataset('re', config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size = BATCH_SIZE,\n",
        "                                               num_workers = NUM_WORKERS,\n",
        "                                               shuffle = True,\n",
        "                                               drop_last = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                         batch_size = BATCH_SIZE,\n",
        "                                         num_workers = NUM_WORKERS,\n",
        "                                         shuffle = False,\n",
        "                                         drop_last = False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                         batch_size = BATCH_SIZE,\n",
        "                                         num_workers = NUM_WORKERS,\n",
        "                                         shuffle = False,\n",
        "                                         drop_last = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "vit_model = transformers.ViTModel.from_pretrained('WinKawaks/vit-small-patch16-224').to(DEVICE)\n",
        "bert_model = transformers.BertForMaskedLM.from_pretrained(\"prajjwal1/bert-small\").to(DEVICE)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
        "\n",
        "model = MAMO(\n",
        "            vit = vit_model,\n",
        "            bert = bert_model,\n",
        "            vit_num_patches= 196,\n",
        "            vit_emb_dim=384,\n",
        "            bert_emb_dim=512,\n",
        "            bert_layers=3,\n",
        "            vocab_size=tokenizer.vocab_size,\n",
        "            mask_token_id= tokenizer.mask_token_id,\n",
        "            # cls_token_id=tokenizer.cls_token_id\n",
        "            ).train().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chkpt = torch.load(weights_path, map_location=DEVICE)['online_model_state_dict']\n",
        "model.load_state_dict(chkpt)\n",
        "\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#optimiser\n",
        "optim = torch.optim.AdamW(model.parameters(),\n",
        "                          lr = lr,\n",
        "                          weight_decay = decay,\n",
        "                          betas = [beta1, beta2],\n",
        "                          )\n",
        "\n",
        "epoch_steps = math.ceil(len(train_dataset)/BATCH_SIZE)\n",
        "num_steps = int(EPOCHS * epoch_steps)\n",
        "warmup_steps = int(warmup_epochs * epoch_steps)\n",
        "\n",
        "lr_scheduler = CosineLRScheduler(\n",
        "        optim,\n",
        "        t_initial=num_steps,\n",
        "        # t_mul=1.,\n",
        "        lr_min=min_lr,\n",
        "        warmup_lr_init = init_lr,\n",
        "        warmup_t=warmup_steps,\n",
        "        cycle_limit=1,\n",
        "        t_in_epochs=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wandB init\n",
        "wandb.init(\n",
        "    id = id,# id,\n",
        "    resume =  'allow',\n",
        "    project = 'MAMO - Finetuning',\n",
        "    name = 'MAMO - ViT-S, BERT-S',\n",
        "\n",
        "    config = {\n",
        "        'architecture': model_name,\n",
        "        'dataset':'ImageNet1K',\n",
        "        'warmup_epochs': warmup_epochs,\n",
        "        'epochs' : EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'masking_ratio_img' : 0.25,\n",
        "        'masking_ratio_itxt' : 0.75,\n",
        "        'mask_patch_size': 196,\n",
        "        'image_size' : DIMENSION,\n",
        "        'optim_params':{\n",
        "            'optim': 'AdamW',\n",
        "            'beta1': beta1,\n",
        "            'beta2': beta2,\n",
        "            'weight_decay': decay,\n",
        "            'learning_rate': lr,\n",
        "        },\n",
        "        'accumulation_iters': 1,\n",
        "        'patch_size_mask' : 32,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "nums = [re.match(r'.*checkpoint_(.*).pth', x).group(1) for x in glob.glob(MODEL_SAVE_PATH+'*.pth')]\n",
        "# if len(nums) > 0:\n",
        "#     nums.remove(\"final\")\n",
        "nums = [int(x) for x in nums]\n",
        "\n",
        "CHKPT = -1\n",
        "\n",
        "if len(nums) != 0:\n",
        "    CHKPT = max(nums)\n",
        "\n",
        "    load_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, CHKPT)\n",
        "    chkpt = torch.load(load_path, map_location = {'cuda:1': device, \n",
        "                                                  'cuda:0': device})\n",
        "\n",
        "    model.load_state_dict(chkpt['model_state_dict'])\n",
        "    optim.load_state_dict(chkpt['optim_state_dict'])\n",
        "    # lr_scheduler.load_state_dict(chkpt['scheduler_state_dict'])\n",
        "    \n",
        "    print(load_path)\n",
        "    \n",
        "    print(\"loaded earlier settings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAJCklE-n0jS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9062 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.3212237213888476:   0%|          | 31/9062 [00:08<42:36,  3.53it/s]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(net_loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# BACKPROP\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# fp16\u001b[39;00m\n\u001b[1;32m     56\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()           \u001b[38;5;66;03m# fp16\u001b[39;00m\n\u001b[1;32m     57\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.11/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.11/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.11/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "scaler = torch.cuda.amp.grad_scaler.GradScaler()\n",
        "itm_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(CHKPT+1, EPOCHS + warmup_epochs):\n",
        "    num_samples = 0\n",
        "    ft_loss = 0\n",
        "    # net all losses\n",
        "    net_itc_loss = 0\n",
        "    net_itm_loss = 0\n",
        "    for idx, data in (pbar := tqdm(enumerate(train_dataloader), total = len(train_dataloader))):\n",
        "        img, txt, img_idx= data\n",
        "        text_input = tokenizer(txt, padding='max_length', truncation=True, max_length=35, return_tensors=\"pt\").to(device)\n",
        "        txt, attn_mask = text_input.input_ids, text_input.attention_mask\n",
        "        # vision\n",
        "        img = img.to(DEVICE)\n",
        "\n",
        "        # language\n",
        "        txt = txt.to(DEVICE)\n",
        "        attn_mask = attn_mask.to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "        # masked modeling real training\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):                           # casting to f16\n",
        "            # forward step for online network\n",
        "            img_rep, txt_rep, joint_rep, img_txt_matching = model(img,\n",
        "                                                                txt,\n",
        "                                                                attn_mask,\n",
        "                                                                retrieval = True)\n",
        "            # ITC loss\n",
        "            sim, itc_loss = model.get_itc_loss(img_rep, txt_rep)\n",
        "\n",
        "            #itm loss\n",
        "            # sample for each image and each text separately\n",
        "            img_maps, txt_maps = model.get_samples(sim)\n",
        "            right_samples = torch.arange(0, img_maps.size(0)).to(DEVICE)\n",
        "\n",
        "            labs_img = (img_maps == right_samples).float().unsqueeze(1)\n",
        "            labs_txt = (txt_maps == right_samples).float().unsqueeze(1)\n",
        "\n",
        "            outs_img = model(img, txt[txt_maps], attn_mask[txt_maps], image_text_matching = True)[-1]\n",
        "            outs_txt = model(img[img_maps], txt, attn_mask, image_text_matching = True)[-1]\n",
        "\n",
        "            # softmax probabilities\n",
        "            itm_1 = itm_loss_fn(outs_img, labs_img)\n",
        "            itm_2 = itm_loss_fn(outs_txt, labs_txt)\n",
        "            itm_loss = itm_1 + itm_2\n",
        "\n",
        "            # TOTAL LOSS\n",
        "            net_loss = (itc_loss) + (itm_loss)\n",
        "\n",
        "        scaler.scale(net_loss).backward()\n",
        "\n",
        "        # BACKPROP\n",
        "        scaler.step(optim)        # fp16\n",
        "        scaler.update()           # fp16\n",
        "        optim.zero_grad(set_to_none = True)\n",
        "        lr_scheduler.step_update(epoch * epoch_steps + idx)\n",
        "\n",
        "        # update and calc loss\n",
        "        num_samples+=1\n",
        "\n",
        "        net_itc_loss+= itc_loss.item()\n",
        "        net_itm_loss+= itm_loss.item()\n",
        "        ft_loss+= net_loss.item()\n",
        "        pbar.set_description(f\"Train Loss: {ft_loss/num_samples}\")\n",
        "\n",
        "    train_stats = {'train_loss': ft_loss,\n",
        "                   'itc_loss': net_itc_loss,\n",
        "                   'itm_loss': net_itm_loss}    \n",
        "\n",
        "    # VALIDATION\n",
        "    score_val_i2t, score_val_t2i, = evaluation(model, val_loader, tokenizer, DEVICE, k=5)\n",
        "    score_test_i2t, score_test_t2i = evaluation(model, test_loader, tokenizer, DEVICE, k=5)\n",
        "\n",
        "\n",
        "    val_result = itm_eval(score_val_i2t, score_val_t2i, val_loader.dataset.txt2img, val_loader.dataset.img2txt)\n",
        "    test_result = itm_eval(score_test_i2t, score_test_t2i, test_loader.dataset.txt2img, test_loader.dataset.img2txt)\n",
        "\n",
        "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                    **{f'val_{k}': v for k, v in val_result.items()},\n",
        "                    **{f'test_{k}': v for k, v in test_result.items()},\n",
        "                    'epoch': epoch,\n",
        "                }\n",
        "    \n",
        "    save_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, epoch)\n",
        "    save_obj = {\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optim.state_dict(),\n",
        "        # 'lr_scheduler': lr_scheduler.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    torch.save(save_obj, save_path)\n",
        "    if (epoch-warmup_epochs+1) % 15 == 0:\n",
        "        wandb.save(save_path)\n",
        "    wandb.log(log_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">MAMO - ViT-S, BERT-S</strong> at: <a href='https://wandb.ai/madhava20217/MAMO%20-%20Finetuning/runs/gk827qfr' target=\"_blank\">https://wandb.ai/madhava20217/MAMO%20-%20Finetuning/runs/gk827qfr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240310_091334-gk827qfr/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
