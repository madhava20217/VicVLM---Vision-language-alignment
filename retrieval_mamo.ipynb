{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1r7bAUV0X-7"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torchinfo import summary\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import tokenizers\n",
        "import itertools\n",
        "\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "\n",
        "from utils.MAMO import MAMO\n",
        "from utils.dataset import re_train_dataset, re_eval_dataset\n",
        "\n",
        "import wandb\n",
        "import nltk\n",
        "\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
        "nltk.download('stopwords')\n",
        "\n",
        "device = 'cuda:0'\n",
        "\n",
        "DEVICE = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
        "model_name = 'ViT-S BERT-S (fixed everything)'\n",
        "algo = 'MAMO'\n",
        "\n",
        "# fix the seed for reproducibility\n",
        "seed = 6969\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "id = wandb.util.generate_id()\n",
        "wandb.login()\n",
        "\n",
        "NUM_WORKERS = 8\n",
        "torch.set_num_threads(12)\n",
        "\n",
        "# set earlier ID\n",
        "# id = ''\n",
        "\n",
        "id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_LENGTH = 30\n",
        "BATCH_SIZE = 144\n",
        "DIMENSION = 224\n",
        "EPOCHS = 15\n",
        "warmup_epochs = 3\n",
        "\n",
        "INTERVAL = 5\n",
        "\n",
        "EVAL_AT = 128\n",
        "\n",
        "lr = 1e-4\n",
        "init_lr = 1e-6\n",
        "min_lr = 1e-5\n",
        "decay = 0.01\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "\n",
        "n_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YBytuU6NIii"
      },
      "outputs": [],
      "source": [
        "weights_path = f'Models/{model_name}/MAMO/checkpoint_final.pth'\n",
        "\n",
        "MODEL_SAVE_PATH = MODEL_SAVE_PATH = f'Finetuning/{model_name}/{algo}/checkpoint'\n",
        "if os.path.exists(os.path.dirname(MODEL_SAVE_PATH)) == False:\n",
        "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wii4D6Dnwmey"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader, tokenizer, device, k=10, max_len = 30):\n",
        "    # test; k for top-k; tokenizer is model.bert\n",
        "    model.eval()\n",
        "\n",
        "    texts = data_loader.dataset.text\n",
        "    num_text = len(texts)\n",
        "    text_bs = 64\n",
        "    text_feats = []\n",
        "    text_embeds = []\n",
        "    text_atts = []\n",
        "    for i in range(0, num_text, text_bs):\n",
        "        text = texts[i: min(num_text, i+text_bs)]\n",
        "        text_input = tokenizer(text, padding='max_length', truncation=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
        "        text_output = model.bert(text_input.input_ids, text_input.attention_mask)\n",
        "        text_feat = text_output['last_hidden_state']                        #unnormalized\n",
        "        text_embed = model.txt_proj(model.pooler(text_feat.transpose(1,2)))\n",
        "        text_embeds.append(text_embed)\n",
        "        text_feats.append(text_feat)\n",
        "        text_atts.append(text_input.attention_mask)\n",
        "    text_embeds = torch.cat(text_embeds,dim=0)\n",
        "    text_feats = torch.cat(text_feats,dim=0)\n",
        "    text_atts = torch.cat(text_atts,dim=0)\n",
        "\n",
        "    image_feats = []\n",
        "    image_embeds = []\n",
        "    for image, img_id in data_loader:\n",
        "        image = image.to(device)\n",
        "        image_feat = model.vit(image)['last_hidden_state']                  #unnormalized \n",
        "        image_embed = model.img_proj(model.pooler(image_feat.transpose(1, 2)))\n",
        "\n",
        "        image_feats.append(image_feat)\n",
        "        image_embeds.append(image_embed)\n",
        "\n",
        "    image_feats = torch.cat(image_feats,dim=0)\n",
        "    image_embeds = torch.cat(image_embeds,dim=0)\n",
        "\n",
        "    sims_matrix = image_embeds @ text_embeds.t()\n",
        "    score_matrix_i2t = torch.full((len(data_loader.dataset.image),len(texts)),-100.0).to(device)\n",
        "\n",
        "\n",
        "    for i,sims in enumerate(sims_matrix):\n",
        "        topk_sim, topk_idx = sims.topk(k=k, dim=0)\n",
        "\n",
        "        encoder_output = image_feats[i].repeat(k,1,1)\n",
        "        # encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
        "        output = model.mamo(encoder_output,\n",
        "                            text_feats[topk_idx],\n",
        "                            text_atts[topk_idx])['last_hidden_state']\n",
        "        \n",
        "        score = model.itm__head(output[:,0,:])#[:,1]          # take output for prediction head 1\n",
        "        score_matrix_i2t[i,topk_idx] = score.squeeze(1)\n",
        "\n",
        "    sims_matrix = sims_matrix.t()\n",
        "    score_matrix_t2i = torch.full((len(texts),len(data_loader.dataset.image)),-100.0).to(device)\n",
        "\n",
        "    for i,sims in enumerate(sims_matrix):\n",
        "        topk_sim, topk_idx = sims.topk(k=k, dim=0)\n",
        "        encoder_output = image_feats[topk_idx]\n",
        "        # encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
        "        output = model.mamo(encoder_output,\n",
        "                                    text_feats[i].repeat(k,1,1),\n",
        "                                    text_atts[i].repeat(k,1))['last_hidden_state']\n",
        "        score = model.itm__head(output[:,0,:])#[:,1]\n",
        "        score_matrix_t2i[i,topk_idx] = score.squeeze(1)\n",
        "\n",
        "    return score_matrix_i2t.cpu().numpy(), score_matrix_t2i.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def itm_eval(scores_i2t, scores_t2i, txt2img, img2txt):\n",
        "\n",
        "    #Images->Text\n",
        "    ranks = np.zeros(scores_i2t.shape[0])\n",
        "    for index,score in enumerate(scores_i2t):\n",
        "        inds = np.argsort(score)[::-1]\n",
        "        # Score\n",
        "        rank = 1e20\n",
        "        for i in img2txt[index]:\n",
        "            tmp = np.where(inds == i)[0][0]\n",
        "            if tmp < rank:\n",
        "                rank = tmp\n",
        "        ranks[index] = rank\n",
        "\n",
        "    # Compute metrics\n",
        "    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
        "    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
        "    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
        "\n",
        "    #Text->Images\n",
        "    ranks = np.zeros(scores_t2i.shape[0])\n",
        "\n",
        "    for index,score in enumerate(scores_t2i):\n",
        "        inds = np.argsort(score)[::-1]\n",
        "        ranks[index] = np.where(inds == txt2img[index])[0][0]\n",
        "\n",
        "    # Compute metrics\n",
        "    ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
        "    ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
        "    ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
        "\n",
        "    tr_mean = (tr1 + tr5 + tr10) / 3\n",
        "    ir_mean = (ir1 + ir5 + ir10) / 3\n",
        "    r_mean = (tr_mean + ir_mean) / 2\n",
        "\n",
        "    eval_result =  {'txt_r1': tr1,\n",
        "                    'txt_r5': tr5,\n",
        "                    'txt_r10': tr10,\n",
        "                    'txt_r_mean': tr_mean,\n",
        "                    'img_r1': ir1,\n",
        "                    'img_r5': ir5,\n",
        "                    'img_r10': ir10,\n",
        "                    'img_r_mean': ir_mean,\n",
        "                    'r_mean': r_mean}\n",
        "    return eval_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQhQuMVd04K8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import v2\n",
        "from PIL import Image\n",
        "\n",
        "def create_dataset(dataset, config):\n",
        "\n",
        "    ## image transforms\n",
        "    train_transform = v2.Compose([\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.int8, scale = True),\n",
        "        v2.Resize(size = (DIMENSION, DIMENSION), antialias = False),\n",
        "        v2.RandAugment(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(\n",
        "            mean = [0.5, 0.5, 0.5],\n",
        "            std =  [0.5, 0.5, 0.5]\n",
        "        )\n",
        "    ])\n",
        "    test_transform = v2.Compose([\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.int8, scale = True),\n",
        "        v2.Resize(size = (DIMENSION, DIMENSION), antialias = False),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(\n",
        "            mean = [0.5, 0.5, 0.5],\n",
        "            std =  [0.5, 0.5, 0.5]\n",
        "        )\n",
        "        ])\n",
        "\n",
        "    if dataset=='re':\n",
        "        train_dataset = re_train_dataset(config['train_file'], train_transform, config['image_root'])\n",
        "        val_dataset = re_eval_dataset(config['val_file'], test_transform, config['image_root'])\n",
        "        test_dataset = re_eval_dataset(config['test_file'], test_transform, config['image_root'])\n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def create_loader(datasets, samplers, batch_size, num_workers, is_trains, collate_fns):\n",
        "    loaders = []\n",
        "    for dataset,sampler,bs,n_worker,is_train,collate_fn in zip(datasets,samplers,batch_size,num_workers,is_trains,collate_fns):\n",
        "        if is_train:\n",
        "            shuffle = (sampler is None)\n",
        "            drop_last = True\n",
        "        else:\n",
        "            shuffle = False\n",
        "            drop_last = False\n",
        "        loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=bs,\n",
        "            num_workers=n_worker,\n",
        "            pin_memory=True,\n",
        "            sampler=sampler,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=collate_fn,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        loaders.append(loader)\n",
        "    return loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {'train_file': ['Jsons/flickr30k_train.json'],\n",
        "          'val_file': 'Jsons/flickr30k_val.json',\n",
        "          'test_file': 'Jsons/flickr30k_test.json',\n",
        "          'image_root': './',\n",
        "          'image_res': DIMENSION}\n",
        "\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_dataset('re', config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size = BATCH_SIZE,\n",
        "                                               num_workers = NUM_WORKERS,\n",
        "                                               shuffle = True,\n",
        "                                               drop_last = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                         batch_size = BATCH_SIZE,\n",
        "                                         num_workers = NUM_WORKERS,\n",
        "                                         shuffle = False,\n",
        "                                         drop_last = False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                         batch_size = BATCH_SIZE,\n",
        "                                         num_workers = NUM_WORKERS,\n",
        "                                         shuffle = False,\n",
        "                                         drop_last = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vit_model = transformers.ViTModel.from_pretrained('WinKawaks/vit-small-patch16-224').to(DEVICE)\n",
        "bert_model = transformers.BertForMaskedLM.from_pretrained(\"prajjwal1/bert-small\").to(DEVICE)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
        "\n",
        "model = MAMO(\n",
        "            vit = vit_model,\n",
        "            bert = bert_model,\n",
        "            vit_num_patches= 196,\n",
        "            vit_emb_dim=384,\n",
        "            bert_emb_dim=512,\n",
        "            bert_layers=n_layers,\n",
        "            vocab_size=tokenizer.vocab_size,\n",
        "            mask_token_id= tokenizer.mask_token_id,\n",
        "            # cls_token_id=tokenizer.cls_token_id\n",
        "            ).train().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chkpt = torch.load(weights_path, map_location=DEVICE)\n",
        "model.load_state_dict(chkpt)\n",
        "\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#optimiser\n",
        "optim = torch.optim.AdamW(model.parameters(),\n",
        "                          lr = lr,\n",
        "                          weight_decay = decay,\n",
        "                          betas = [beta1, beta2],\n",
        "                          )\n",
        "\n",
        "epoch_steps = math.ceil(len(train_dataset)/BATCH_SIZE)\n",
        "num_steps = int(EPOCHS * epoch_steps)\n",
        "warmup_steps = int(warmup_epochs * epoch_steps)\n",
        "\n",
        "lr_scheduler = CosineLRScheduler(\n",
        "        optim,\n",
        "        t_initial=num_steps,\n",
        "        # t_mul=1.,\n",
        "        lr_min=min_lr,\n",
        "        warmup_lr_init = init_lr,\n",
        "        warmup_t=warmup_steps,\n",
        "        cycle_limit=1,\n",
        "        t_in_epochs=False,\n",
        "    )"
      ]
    },
    