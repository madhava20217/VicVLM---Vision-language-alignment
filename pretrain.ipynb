{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "import torchtext\n",
    "\n",
    "device = 'cuda:0'\n",
    "n_workers = 8\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import glob\n",
    "\n",
    "torch.set_num_threads(n_workers)\n",
    "\n",
    "DEVICE = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_name = 'vit_bert_s'\n",
    "algo = 'MAMO'\n",
    "\n",
    "import tokenizers\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SRC = '../Datasets/Flickr30k/'\n",
    "MODEL_SAVE_PATH = f'Models/{model_name}/{algo}/checkpoint'\n",
    "\n",
    "VOCAB_PATH = 'Vocabulary/flickr30k.vocab'\n",
    "\n",
    "n_layers = 2\n",
    "\n",
    "if os.path.exists(os.path.dirname(MODEL_SAVE_PATH)) == False:\n",
    "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(0, num_layers_to_keep):\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "def deleteLaterEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(num_layers_to_keep, 0, -1):\n",
    "        newModuleList.append(oldModuleList[-i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "\n",
    "def get_bert_model(model, num_layers):\n",
    "    return deleteEncodingLayers(model, num_layers)\n",
    "\n",
    "\n",
    "\n",
    "class MAMO_mixer(torch.nn.Module):\n",
    "    def __init__(self, base_bert, n_layers = 2, n_visual_tokens = 197, vision_embedding_dim = 384, emb_dims = 512):\n",
    "        # prepare decoder\n",
    "        super().__init__()\n",
    "        self.n_visual_tokens = n_visual_tokens\n",
    "        self.vision_emb_dim = vision_embedding_dim\n",
    "        self.base_model = deleteLaterEncodingLayers(base_bert.base_model, n_layers).encoder\n",
    "        \n",
    "        self.pooler = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.emb_dimension = emb_dims\n",
    "        \n",
    "        if self.vision_emb_dim == self.emb_dimension:\n",
    "            self.dimension_caster = torch.nn.Identity()\n",
    "        else:\n",
    "            self.dimension_caster = torch.nn.Linear(self.vision_emb_dim, self.emb_dimension, bias = False)  # no bias here\n",
    "        \n",
    "        \n",
    "    def forward(self, vision_embedding, text_embedding, text_attn_mask):\n",
    "        # assert len(vision_embedding) == len(text_embedding)\n",
    "        n_batch = len(vision_embedding)\n",
    "        \n",
    "        # normalize dimensions\n",
    "        new_vision_emb = self.dimension_caster(vision_embedding)\n",
    "        \n",
    "        # concatenate\n",
    "        concatenated_emb = torch.cat([new_vision_emb, text_embedding], dim = 1)\n",
    "        \n",
    "        # create attention mask\n",
    "        vision_attention_mask = torch.ones(n_batch, self.n_visual_tokens).to(text_attn_mask.device)\n",
    "        attn_mask = torch.cat([vision_attention_mask, text_attn_mask], dim = 1)\n",
    "        \n",
    "        attn_mask = attn_mask[:, None, None, :]\n",
    "        \n",
    "        # forward\n",
    "        return self.base_model(concatenated_emb, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DIMENSION = 224\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "# ViT config\n",
    "feature_extractor = transformers.AutoFeatureExtractor.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "## image transforms\n",
    "img_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.int8, scale = True),\n",
    "    v2.RandomResizedCrop(size = (DIMENSION, DIMENSION), \n",
    "                                scale = [0.67,1], \n",
    "                                ratio = [3/4, 4/3],\n",
    "                                antialias = False),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean = [0.5, 0.5, 0.5],\n",
    "        std =  [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class MaskGenerator:\n",
    "    def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):\n",
    "        self.input_size = input_size\n",
    "        self.mask_patch_size = mask_patch_size\n",
    "        self.model_patch_size = model_patch_size\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "        assert self.input_size % self.mask_patch_size == 0\n",
    "        assert self.mask_patch_size % self.model_patch_size == 0\n",
    "        \n",
    "        self.rand_size = self.input_size // self.mask_patch_size\n",
    "        self.scale = self.mask_patch_size // self.model_patch_size\n",
    "        \n",
    "        self.token_count = self.rand_size ** 2\n",
    "        self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))\n",
    "        \n",
    "    def __call__(self):\n",
    "        mask_idx = np.random.permutation(self.token_count)[:self.mask_count]\n",
    "        mask = np.zeros(self.token_count, dtype=int)\n",
    "        mask[mask_idx] = 1\n",
    "        \n",
    "        mask = mask.reshape((self.rand_size, self.rand_size))\n",
    "        mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "unfold_dim = int(DIMENSION/16)\n",
    "fold_params = {'kernel_size' : unfold_dim,\n",
    "              'dilation': 16}\n",
    "# utils\n",
    "def get_patches(batch, num_patches, size_patch):\n",
    "    '''Function to get patches from an image\n",
    "    \n",
    "    Arguments:\n",
    "    1. batch: batch of tensor images (N, C, H, W)\n",
    "    2. num_patches: number of patches per side\n",
    "    3. size_patch: number of pixels along each side of the patch\n",
    "    \n",
    "    Returns:\n",
    "    Tensor containing patch-wise representation of each image in the batch\n",
    "    '''\n",
    "    return torch.nn.functional.unfold(\n",
    "        batch,\n",
    "        **fold_params\n",
    "        ).reshape(batch.size(0), -1, num_patches**2, size_patch**2).permute(0,2,1,3)\n",
    "\n",
    "def get_img_from_patches(batch, num_patches, size_patch):\n",
    "    '''Function to reconstruct an image from its patches\n",
    "    \n",
    "    Arguments:\n",
    "    1. batch: batch of tensor images (N, C, H, W)\n",
    "    2. num_patches: number of patches per side\n",
    "    3. size_patch: number of pixels along each side of the patch\n",
    "    \n",
    "    Returns:\n",
    "    tensor containing reconstructed images'''\n",
    "    # return fold(batch.permute(0,2,1,3).reshape(batch.size(0), -1, size_patch**2))\n",
    "    return torch.nn.functional.fold(\n",
    "        batch.permute(0,2,1,3).reshape(batch.size(0), -1, size_patch**2),\n",
    "        output_size = (DIMENSION, DIMENSION),\n",
    "        **fold_params\n",
    "    )\n",
    "    \n",
    "class TextMaskGenerator:\n",
    "    def __init__(self, masking_ratio = 0.25, mask_token = '[MASK]'):\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.mask_token = mask_token\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        text = np.array(text.split())  # tokenized\n",
    "        len_txt = len(text)\n",
    "        \n",
    "        n_to_mask = math.ceil(len_txt * self.masking_ratio)\n",
    "        rankings = np.random.randn(len_txt)\n",
    "        \n",
    "        indices = np.argpartition(rankings, -n_to_mask)[-n_to_mask:]\n",
    "        text[indices] = self.mask_token\n",
    "        \n",
    "        \n",
    "        \n",
    "        return \" \".join(text)\n",
    "        \n",
    "\n",
    "class Flickr30K_MAMO(torchvision.datasets.Flickr30k):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 ann_path,\n",
    "                 img_transform = None,\n",
    "                 txt_transform = None,\n",
    "                 max_length = 100,\n",
    "                 ):\n",
    "        super().__init__(data_path, ann_path)\n",
    "        self.img_transform = img_transform\n",
    "        self.tokenizer = txt_transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.img_masker = MaskGenerator(input_size = 224,\n",
    "                                        mask_patch_size = 32,\n",
    "                                        model_patch_size = 16,\n",
    "                                        mask_ratio = 0.75)       #0.75 masking ratio with MAMO\n",
    "        \n",
    "        self.txt_masker = TextMaskGenerator(masking_ratio=0.25,\n",
    "                                            mask_token = self.tokenizer.mask_token)\n",
    "        \n",
    "        \n",
    "    def process_string(self, string):\n",
    "        tok_str = string.lower().split() # separated by spaces\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        proc_str = [x for x in tok_str if x not in stopwords]   # stopword removal\n",
    "        \n",
    "        return \" \".join(proc_str)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, txt = super().__getitem__(idx)\n",
    "        txt = random.choice(txt)\n",
    "        \n",
    "        \n",
    "        # get images and texts\n",
    "        img = self.img_transform(img)\n",
    "        \n",
    "        # process string\n",
    "        txt = self.process_string(txt)\n",
    "        mask_txt = self.txt_masker(txt)\n",
    "        \n",
    "        tok_text = self.tokenizer(txt, truncation = True, padding = 'max_length', max_length = self.max_length, return_token_type_ids=False)\n",
    "        tok_masked_txt = self.tokenizer(mask_txt, truncation = True, padding = 'max_length', max_length = self.max_length, return_token_type_ids=False)\n",
    "        toks, attn_mask = tok_text['input_ids'], tok_text['attention_mask']\n",
    "        masked_toks, masked_attn_mask = tok_masked_txt['input_ids'], tok_masked_txt['attention_mask']\n",
    "        \n",
    "        toks, attn_mask, masked_toks, masked_attn_mask = torch.tensor(toks), torch.tensor(attn_mask), torch.tensor(masked_toks), torch.tensor(masked_attn_mask)\n",
    "        \n",
    "        # masked indices\n",
    "        mask_indices = (masked_toks == tokenizer.mask_token_id)\n",
    "        \n",
    "        # generate mask for image and text\n",
    "        img_mask = self.img_masker()\n",
    "        \n",
    "        return img, img_mask, toks, attn_mask, masked_toks, masked_attn_mask, mask_indices\n",
    "        \n",
    "\n",
    "def create_masked_image(img, mask):\n",
    "    mask = (mask .repeat_interleave(16, 1)\n",
    "                .repeat_interleave(16, 2)\n",
    "                .unsqueeze(1)\n",
    "                .contiguous()\n",
    "            )\n",
    "    return (img * (1 - mask))\n",
    "      \n",
    "        \n",
    "dataset = Flickr30K_MAMO(DATASET_SRC + 'flickr30k-images',\n",
    "               DATASET_SRC + 'results_20130124.token',\n",
    "               img_transform=img_transform,\n",
    "               txt_transform=tokenizer,\n",
    "               max_length = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 32,\n",
    "    pin_memory = True,\n",
    "    # num_workers = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 30522)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMO(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 vit,\n",
    "                 bert,\n",
    "                 vit_num_patches = 196,\n",
    "                 vit_emb_dim = 384,\n",
    "                 bert_emb_dim = 512,\n",
    "                 bert_layers = 2,\n",
    "                 vocab_size = 30522,\n",
    "                 mask_token_id = 103):\n",
    "       super().__init__()\n",
    "       self.vit = vit\n",
    "       self.bert = bert.base_model\n",
    "       self.bert = deleteEncodingLayers(self.bert.base_model, bert_layers)\n",
    "       self.mamo = MAMO_mixer(bert, bert_layers, 197, vit_emb_dim)\n",
    "       \n",
    "       # vit patches data\n",
    "       self.vit_num_patches = vit_num_patches\n",
    "       \n",
    "       # vocab size\n",
    "       self.vocab_size = vocab_size\n",
    "       # mask token\n",
    "       self.mask_token_id = mask_token_id\n",
    "       \n",
    "       # learnable temperature parameter\n",
    "       self.tau = torch.nn.Parameter(torch.randn(1)*5)\n",
    "       self.tau.requires_grad = True\n",
    "\n",
    "       # joint representation\n",
    "       self.pooler = torch.nn.Sequential(\n",
    "           torch.nn.AdaptiveAvgPool1d(1),\n",
    "           torch.nn.Flatten()\n",
    "       )\n",
    "       self.img_proj = torch.nn.Linear(vit_emb_dim, min(vit_emb_dim, bert_emb_dim))\n",
    "       self.txt_proj = torch.nn.Linear(bert_emb_dim, min(vit_emb_dim, bert_emb_dim))\n",
    "\n",
    "       \n",
    "       # masked representation modeling\n",
    "       self.mrm_proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bert_emb_dim, bert_emb_dim),\n",
    "            torch.nn.Tanh(),\n",
    "       )\n",
    "       \n",
    "       # head for masked image modeling\n",
    "       self.mim_proj = torch.nn.Sequential(\n",
    "           torch.nn.Linear(bert_emb_dim, vit_emb_dim),\n",
    "       )\n",
    "        \n",
    "       # head for masked language modeling\n",
    "       self.mlm_head = bert.cls\n",
    "    \n",
    "       \n",
    "       \n",
    "    def forward(self, image, text, attn_mask,\n",
    "                masked_modeling = True,\n",
    "                masked_image = None,\n",
    "                masked_text = None,\n",
    "                ):\n",
    "        \n",
    "        if not masked_modeling:\n",
    "            img_rep = self.vit(image)['last_hidden_state']\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']\n",
    "            joint_rep = self.mamo(img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "        \n",
    "            return (img_rep, txt_rep, joint_rep)\n",
    "        \n",
    "        else:\n",
    "            # return mask_img-clean_txt, clean_img,-mask_txt, \n",
    "            img_rep = self.vit(image)['last_hidden_state']              # clean image\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']   # clean text\n",
    "            \n",
    "            mask_img_rep = self.vit(masked_image)['last_hidden_state']\n",
    "            mask_txt_rep = self.bert(masked_text, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            # multimodal prediction\n",
    "            c_img_m_txt = self.mamo(img_rep, mask_txt_rep, attn_mask)['last_hidden_state']\n",
    "            m_img_c_txt = self.mamo(mask_img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "        \n",
    "            # pure txt\n",
    "            txt_prediction = self.mlm_head(mask_txt_rep)\n",
    "            \n",
    "            # pool and flatten text and visual features obtained before fusion\n",
    "            img_rep = self.img_proj(self.pooler(img_rep.transpose(1,2)))\n",
    "            txt_rep = self.txt_proj(self.pooler(txt_rep.transpose(1,2)))\n",
    "            \n",
    "            return (c_img_m_txt, m_img_c_txt, mask_img_rep, txt_prediction, img_rep, txt_rep)\n",
    "            \n",
    "    def mrm_projection(self, rep):\n",
    "        return self.mrm_proj(rep)\n",
    "    \n",
    "    def mim_projection(self, rep):\n",
    "        return self.mim_proj(rep)\n",
    "    \n",
    "    def get_mrm_loss(self, online_representation, target_representation, mask):\n",
    "        # remove cls token\n",
    "        on_rep = self.mrm_projection(online_representation[:, 1:, :])\n",
    "        tr_rep = target_representation[:, 1:, :]\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(on_rep, tr_rep, reduction = 'none')\n",
    "        mrm_loss = (loss * mask).sum()/(mask.sum() + 1e-5)              # add for 0 division errors\n",
    "        return mrm_loss\n",
    "    \n",
    "    def get_mim_loss(self, online_representation, target_representation, mask):\n",
    "        on_rep = self.mim_projection(online_representation[:, 1:self.vit_num_patches+1, :]) # omit cls token\n",
    "        tr_rep = target_representation[:, 1:self.vit_num_patches+1, :]\n",
    "        \n",
    "        loss = torch.nn.functional.l1_loss(on_rep, tr_rep, reduction = 'none')\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask[:, :, None]\n",
    "        mim_loss = (loss * mask).sum() / (mask.sum() + 1e-5)\n",
    "        return mim_loss \n",
    "    \n",
    "    \n",
    "    def get_mlm_loss(self, scores, target, mask):\n",
    "        labels = torch.where(target == self.mask_token_id, target, -100)\n",
    "        loss = torch.nn.functional.cross_entropy(scores.view(-1, self.vocab_size), labels.view(-1))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def get_itc_loss(self, img_feats, txt_feats):\n",
    "        # Calculate cosine similarity\n",
    "        sim = torch.exp((img_feats@txt_feats.T)/self.tau)\n",
    "        self_mask = torch.eye(sim.shape[0], device=sim.device)\n",
    "\n",
    "        return (torch.nn.functional.cross_entropy(sim, self_mask) + torch.nn.functional.cross_entropy(sim.T, self_mask))/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vit_model = transformers.ViTModel.from_pretrained('WinKawaks/vit-small-patch16-224').to(DEVICE)\n",
    "bert_model = transformers.BertForMaskedLM.from_pretrained(\"prajjwal1/bert-small\")\n",
    "\n",
    "online_network = MAMO(\n",
    "                    vit = vit_model,\n",
    "                    bert = bert_model,\n",
    "                    vit_num_patches= 196,\n",
    "                    vit_emb_dim=384,\n",
    "                    bert_emb_dim=512,\n",
    "                    bert_layers=2,\n",
    "                    vocab_size=tokenizer.vocab_size,\n",
    "                    mask_token_id= tokenizer.mask_token_id\n",
    "                ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for target network\n",
    "\n",
    "# freeze weights\n",
    "def freeze_weights(nw):\n",
    "    for param in nw.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return nw\n",
    "    \n",
    "def ewma_weights(target, current, alpha = 0.995):\n",
    "    sdA = target.state_dict()\n",
    "    sdB = current.state_dict()\n",
    "    \n",
    "    for key in sdA:\n",
    "        sdA[key] = alpha*sdA[key] + (1-alpha)*sdB[key]\n",
    "    \n",
    "    target.load_state_dict(sdA)\n",
    "    return target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = copy.deepcopy(online_network)\n",
    "target_network = freeze_weights(target_network).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = torch.cuda.amp.grad_scaler.GradScaler()\n",
    "\n",
    "l2_loss = torch.nn.MSELoss()\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "\n",
    "for idx, data in enumerate(pretrain_dataloader):\n",
    "    img, img_mask, txt, attn_mask, masked_toks, masked_attn_mask, mask_indices = data\n",
    "    \n",
    "    # vision\n",
    "    img = img.to(DEVICE)\n",
    "    img_mask = img_mask.to(DEVICE)\n",
    "    \n",
    "    # language\n",
    "    txt = txt.to(DEVICE)\n",
    "    attn_mask = attn_mask.to(DEVICE)\n",
    "    masked_toks = masked_toks.to(DEVICE)\n",
    "    masked_attn_mask = masked_attn_mask.to(DEVICE)\n",
    "\n",
    "    # indices for masked text: will be used for masked modeling\n",
    "    mask_indices = mask_indices.float().to(DEVICE)\n",
    "    \n",
    "    # masked image\n",
    "    masked_image = create_masked_image(img, img_mask)\n",
    "    flattened_img_mask = img_mask.float().flatten(1)\n",
    "    \n",
    "    # create masks for joint representation modeling\n",
    "    img_rep_masks = torch.cat([flattened_img_mask, torch.zeros_like(mask_indices)], axis = 1).unsqueeze(-1)\n",
    "    txt_rep_masks = torch.cat([torch.zeros_like(flattened_img_mask), mask_indices], axis = 1).unsqueeze(-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # masked modeling pretraining\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):                           # casting to f16\n",
    "        \n",
    "        \n",
    "        # forward step for target network\n",
    "        with torch.no_grad():\n",
    "            target_img_rep, target_txt_rep, target_mm_rep = target_network(img, txt, attn_mask, masked_modeling = False)\n",
    "        \n",
    "        # forward step for online network\n",
    "        c_img_m_txt, m_img_c_txt, mask_img_rep, txt_prediction, img_rep, txt_rep = online_network(img, txt, attn_mask,\n",
    "                                                                                    masked_modeling = True,\n",
    "                                                                                    masked_image = masked_image,\n",
    "                                                                                    masked_text = masked_toks)\n",
    "\n",
    "\n",
    "        # MRM loss\n",
    "        mrm_loss_txt = online_network.get_mrm_loss(c_img_m_txt, target_mm_rep, txt_rep_masks)\n",
    "        mrm_loss_img = online_network.get_mrm_loss(m_img_c_txt, target_mm_rep, img_rep_masks)\n",
    "        \n",
    "        # MIM and MLM losses\n",
    "        mim_loss = online_network.get_mim_loss(m_img_c_txt, target_img_rep, flattened_img_mask)\n",
    "        mlm_loss = online_network.get_mlm_loss(txt_prediction, txt, mask_indices)\n",
    "         \n",
    "        \n",
    "        # ITC loss\n",
    "        itc_loss = online_network.get_itc_loss(img_rep, txt_rep)\n",
    "        \n",
    "    break\n",
    "    \n",
    "    # masked image modeling\n",
    "    \n",
    "    \n",
    "    # masked text modeling\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # global image-text alignment\n",
    "    # itc loss : contrastive\n",
    "    \n",
    "    \n",
    "    \n",
    "    # image-text matching : sample according to similarity measure for each example, then do supervised prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7429, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 247, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_network.mrm_projection(c_img_m_txt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 247, 512]),\n",
       " torch.Size([32, 247, 512]),\n",
       " torch.Size([32, 197, 384]),\n",
       " torch.Size([32, 50, 30522]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_img_m_txt.shape, m_img_c_txt.shape, mask_img_rep.shape, txt_prediction.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
