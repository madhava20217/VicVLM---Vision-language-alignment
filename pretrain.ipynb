{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "import torchtext\n",
    "\n",
    "device = 'cuda:0'\n",
    "n_workers = 8\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import glob\n",
    "\n",
    "torch.set_num_threads(n_workers)\n",
    "\n",
    "DEVICE = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_name = 'vit_bert_s'\n",
    "algo = 'MAMO'\n",
    "\n",
    "import tokenizers\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SRC = '../Datasets/Flickr30k/'\n",
    "MODEL_SAVE_PATH = f'Models/{model_name}/{algo}/checkpoint'\n",
    "\n",
    "VOCAB_PATH = 'Vocabulary/flickr30k.vocab'\n",
    "\n",
    "n_layers = 2\n",
    "\n",
    "if os.path.exists(os.path.dirname(MODEL_SAVE_PATH)) == False:\n",
    "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(0, num_layers_to_keep):\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "def deleteLaterEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(num_layers_to_keep, 0, -1):\n",
    "        newModuleList.append(oldModuleList[-i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "\n",
    "def get_bert_model(model, num_layers):\n",
    "    return deleteEncodingLayers(model, num_layers)\n",
    "\n",
    "\n",
    "\n",
    "class MAMO_mixer(torch.nn.Module):\n",
    "    def __init__(self, base_bert, n_layers = 2, n_visual_tokens = 197, vision_embedding_dim = 384):\n",
    "        # prepare decoder\n",
    "        super().__init__()\n",
    "        self.n_visual_tokens = n_visual_tokens\n",
    "        self.vision_emb_dim = vision_embedding_dim\n",
    "        self.emb_dimension = base_bert.pooler.dense.out_features\n",
    "        self.base_model = deleteLaterEncodingLayers(base_bert, n_layers).encoder\n",
    "        \n",
    "        self.pooler = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        if self.vision_emb_dim == self.emb_dimension:\n",
    "            self.dimension_caster = torch.nn.Identity()\n",
    "        else:\n",
    "            self.dimension_caster = torch.nn.Linear(self.vision_emb_dim, self.emb_dimension, bias = False)  # no bias here\n",
    "        \n",
    "        \n",
    "    def forward(self, vision_embedding, text_embedding, text_attn_mask):\n",
    "        # assert len(vision_embedding) == len(text_embedding)\n",
    "        n_batch = len(vision_embedding)\n",
    "        \n",
    "        # normalize dimensions\n",
    "        new_vision_emb = self.dimension_caster(vision_embedding)\n",
    "        \n",
    "        # concatenate\n",
    "        concatenated_emb = torch.cat([new_vision_emb, text_embedding], dim = 1)\n",
    "        \n",
    "        # create attention mask\n",
    "        vision_attention_mask = torch.ones(n_batch, self.n_visual_tokens)\n",
    "        attn_mask = torch.cat([vision_attention_mask, text_attn_mask], dim = 1)\n",
    "        \n",
    "        attn_mask = attn_mask[:, None, None, :]\n",
    "        \n",
    "        # forward\n",
    "        return self.base_model(concatenated_emb, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mamo_model = transformers.BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
    "# mamo_model = MAMO_mixer(mamo_model, 2, 197, 384)#.to(DEVICE)\n",
    "\n",
    "# vision_inp = torch.empty(10, 197, 384)\n",
    "# text_inp = torch.empty(10, 50, 512)\n",
    "# text_attn = torch.randint(0, 1, (10, 50))\n",
    "\n",
    "# mamo_model(vision_inp, text_inp, text_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "DIMENSION = 224\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "# ViT config\n",
    "# image_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# model = transformers.ViTForMaskedImageModeling.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "feature_extractor = transformers.AutoFeatureExtractor.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "vit_model = transformers.ViTModel.from_pretrained('WinKawaks/vit-small-patch16-224').to(DEVICE)\n",
    "\n",
    "# DistilBERT config\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "# bert_model = transformers.BertModel.from_pretrained(\"google-bert/bert-base-uncased\").to(DEVICE)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
    "bert_model = transformers.BertModel.from_pretrained(\"prajjwal1/bert-small\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ViTModel                                           [5, 384]                  --\n",
       "├─ViTEmbeddings: 1-1                               [5, 197, 384]             76,032\n",
       "│    └─ViTPatchEmbeddings: 2-1                     [5, 196, 384]             --\n",
       "│    │    └─Conv2d: 3-1                            [5, 384, 14, 14]          295,296\n",
       "│    └─Dropout: 2-2                                [5, 197, 384]             --\n",
       "├─ViTEncoder: 1-2                                  [5, 197, 384]             --\n",
       "│    └─ModuleList: 2-3                             --                        --\n",
       "│    │    └─ViTLayer: 3-2                          [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-3                          [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-4                          [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-5                          [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-6                          [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-7                          [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-8                          [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-9                          [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-10                         [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-11                         [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-12                         [5, 197, 384]             1,774,464\n",
       "│    │    └─ViTLayer: 3-13                         [5, 197, 384]             1,774,464\n",
       "├─LayerNorm: 1-3                                   [5, 197, 384]             768\n",
       "├─ViTPooler: 1-4                                   [5, 384]                  --\n",
       "│    └─Linear: 2-4                                 [5, 384]                  147,840\n",
       "│    └─Tanh: 2-5                                   [5, 384]                  --\n",
       "====================================================================================================\n",
       "Total params: 21,813,504\n",
       "Trainable params: 21,813,504\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 396.60\n",
       "====================================================================================================\n",
       "Input size (MB): 3.01\n",
       "Forward/backward pass size (MB): 405.47\n",
       "Params size (MB): 86.95\n",
       "Estimated Total Size (MB): 495.43\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vit_model, input_size = (5, 3, DIMENSION, DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dimension of the BERT model to map ViT and BERT to\n",
    "\n",
    "with torch.no_grad():\n",
    "    outs = bert_model(torch.zeros(5, MAX_LEN, dtype=torch.int).to(DEVICE), torch.ones(5, MAX_LEN, dtype = torch.int).to(DEVICE))['last_hidden_state']\n",
    "# uniform_embedding_dimension = outs.shape[-1]\n",
    "# uniform_embedding_dimension, outs.shape\n",
    "emb_dim = outs.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "## image transforms\n",
    "img_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.int8, scale = True),\n",
    "    v2.RandomResizedCrop(size = (DIMENSION, DIMENSION), \n",
    "                                scale = [0.67,1], \n",
    "                                ratio = [3/4, 4/3],\n",
    "                                antialias = False),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean = [0.5, 0.5, 0.5],\n",
    "        std =  [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class MaskGenerator:\n",
    "    def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):\n",
    "        self.input_size = input_size\n",
    "        self.mask_patch_size = mask_patch_size\n",
    "        self.model_patch_size = model_patch_size\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "        assert self.input_size % self.mask_patch_size == 0\n",
    "        assert self.mask_patch_size % self.model_patch_size == 0\n",
    "        \n",
    "        self.rand_size = self.input_size // self.mask_patch_size\n",
    "        self.scale = self.mask_patch_size // self.model_patch_size\n",
    "        \n",
    "        self.token_count = self.rand_size ** 2\n",
    "        self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))\n",
    "        \n",
    "    def __call__(self):\n",
    "        mask_idx = np.random.permutation(self.token_count)[:self.mask_count]\n",
    "        mask = np.zeros(self.token_count, dtype=int)\n",
    "        mask[mask_idx] = 1\n",
    "        \n",
    "        mask = mask.reshape((self.rand_size, self.rand_size))\n",
    "        mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    \n",
    "class TextMaskGenerator:\n",
    "    def __init__(self, masking_ratio = 0.25, mask_token = '[MASK]'):\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.mask_token = mask_token\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        text = np.array(text.split())  # tokenized\n",
    "        len_txt = len(text)\n",
    "        \n",
    "        n_to_mask = math.ceil(len_txt * self.masking_ratio)\n",
    "        rankings = np.random.randn(len_txt)\n",
    "        \n",
    "        indices = np.argpartition(rankings, -n_to_mask)[-n_to_mask:]\n",
    "        text[indices] = self.mask_token\n",
    "        \n",
    "        \n",
    "        \n",
    "        return \" \".join(text)\n",
    "        \n",
    "\n",
    "class Flickr30K_MAMO(torchvision.datasets.Flickr30k):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 ann_path,\n",
    "                 img_transform = None,\n",
    "                 txt_transform = None,\n",
    "                 max_length = 100,\n",
    "                 ):\n",
    "        super().__init__(data_path, ann_path)\n",
    "        self.img_transform = img_transform\n",
    "        self.tokenizer = txt_transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.img_masker = MaskGenerator(input_size = 224,\n",
    "                                        mask_patch_size = 32,\n",
    "                                        model_patch_size = 16,\n",
    "                                        mask_ratio = 0.75)       #0.75 masking ratio with MAMO\n",
    "        \n",
    "        self.txt_masker = TextMaskGenerator(masking_ratio=0.25,\n",
    "                                            mask_token = self.tokenizer.mask_token)\n",
    "        \n",
    "        \n",
    "    def process_string(self, string):\n",
    "        tok_str = string.lower().split() # separated by spaces\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        proc_str = [x for x in tok_str if x not in stopwords]   # stopword removal\n",
    "        \n",
    "        return \" \".join(proc_str)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, txt = super().__getitem__(idx)\n",
    "        txt = random.choice(txt)\n",
    "        \n",
    "        \n",
    "        # get images and texts\n",
    "        img = self.img_transform(img)\n",
    "        \n",
    "        # process string\n",
    "        txt = self.process_string(txt)\n",
    "        mask_txt = self.txt_masker(txt)\n",
    "        \n",
    "        tok_text = self.tokenizer(txt, truncation = True, padding = 'max_length', max_length = self.max_length, return_token_type_ids=False)\n",
    "        tok_masked_txt = self.tokenizer(mask_txt, truncation = True, padding = 'max_length', max_length = self.max_length, return_token_type_ids=False)\n",
    "        toks, attn_mask = tok_text['input_ids'], tok_text['attention_mask']\n",
    "        masked_toks, masked_attn_mask = tok_masked_txt['input_ids'], tok_masked_txt['attention_mask']\n",
    "        \n",
    "        toks, attn_mask, masked_toks, masked_attn_mask = torch.tensor(toks), torch.tensor(attn_mask), torch.tensor(masked_toks), torch.tensor(masked_attn_mask)\n",
    "        \n",
    "        # masked indices\n",
    "        mask_indices = (masked_toks == tokenizer.mask_token_id)\n",
    "        \n",
    "        # generate mask for image and text\n",
    "        img_mask = self.img_masker()\n",
    "        \n",
    "        return img, img_mask, toks, attn_mask, masked_toks, masked_attn_mask, mask_indices\n",
    "        \n",
    "        \n",
    "        \n",
    "dataset = Flickr30K_MAMO(DATASET_SRC + 'flickr30k-images',\n",
    "               DATASET_SRC + 'results_20130124.token',\n",
    "               img_transform=img_transform,\n",
    "               txt_transform=tokenizer,\n",
    "               max_length = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 128,\n",
    "    pin_memory = True,\n",
    "    # num_workers = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for idx, data in enumerate(pretrain_dataloader):\n",
    "    img, img_mask, toks, attn_mask, masked_toks, masked_attn_mask, mask_indices = data\n",
    "    \n",
    "    # vision\n",
    "    img = img.to(DEVICE)\n",
    "    img_mask = img_mask.to(DEVICE)\n",
    "    \n",
    "    # language\n",
    "    toks = toks.to(DEVICE)\n",
    "    attn_mask = attn_mask.to(DEVICE)\n",
    "    masked_toks = masked_toks.to(DEVICE)\n",
    "    masked_attn_mask = masked_attn_mask.to(DEVICE)\n",
    "\n",
    "    mask_indices = mask_indices.to(DEVICE)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # MAMO has 5 components now...\n",
    "    # masked modeling\n",
    "    \n",
    "    \n",
    "    \n",
    "    # masked joint representation modeling\n",
    "    \n",
    "    \n",
    "    # masked image modeling\n",
    "    \n",
    "    \n",
    "    # masked text modeling\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # global image-text alignment\n",
    "    # itc loss : contrastive\n",
    "    \n",
    "    \n",
    "    \n",
    "    # image-text matching : sample according to similarity measure for each example, then do supervised prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
