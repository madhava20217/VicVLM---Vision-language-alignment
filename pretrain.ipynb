{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tokenizers\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "\n",
    "DEVICE = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_name = 'vit_bert_s'\n",
    "algo = 'MAMO'\n",
    "\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "\n",
    "id = wandb.util.generate_id()\n",
    "wandb.login()\n",
    "\n",
    "# set earlier ID\n",
    "id = 'ci3xdvn5'\n",
    "\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SRC = '../Datasets/Flickr30k/'\n",
    "MODEL_SAVE_PATH = f'Models/{model_name}/{algo}/checkpoint'\n",
    "\n",
    "VOCAB_PATH = 'Vocabulary/flickr30k.vocab'\n",
    "\n",
    "lr = 3e-4\n",
    "init_lr = 1e-6\n",
    "min_lr = 1e-5\n",
    "decay = 0.01\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "warmup_epochs = 5\n",
    "EPOCHS = 60\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "MASKING_RATIO_IMG = 0.75\n",
    "MASKING_RATIO_TXT = 0.25\n",
    "\n",
    "ALPHA = 0.995               # EWMA\n",
    "\n",
    "\n",
    "n_layers = 2\n",
    "\n",
    "if os.path.exists(os.path.dirname(MODEL_SAVE_PATH)) == False:\n",
    "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(0, num_layers_to_keep):\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "def deleteLaterEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = torch.nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(num_layers_to_keep, 0, -1):\n",
    "        newModuleList.append(oldModuleList[-i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "\n",
    "def get_bert_model(model, num_layers):\n",
    "    return deleteEncodingLayers(model, num_layers)\n",
    "\n",
    "\n",
    "\n",
    "class MAMO_mixer(torch.nn.Module):\n",
    "    def __init__(self, base_bert, n_layers = 2, n_visual_tokens = 197, vision_embedding_dim = 384, emb_dims = 512):\n",
    "        # prepare decoder\n",
    "        super().__init__()\n",
    "        self.n_visual_tokens = n_visual_tokens\n",
    "        self.vision_emb_dim = vision_embedding_dim\n",
    "        self.base_model = deleteLaterEncodingLayers(base_bert.base_model, n_layers).encoder\n",
    "        \n",
    "        self.pooler = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.emb_dimension = emb_dims\n",
    "        \n",
    "        if self.vision_emb_dim == self.emb_dimension:\n",
    "            self.dimension_caster = torch.nn.Identity()\n",
    "        else:\n",
    "            self.dimension_caster = torch.nn.Linear(self.vision_emb_dim, self.emb_dimension, bias = False)  # no bias here\n",
    "        \n",
    "        \n",
    "    def forward(self, vision_embedding, text_embedding, text_attn_mask):\n",
    "        # assert len(vision_embedding) == len(text_embedding)\n",
    "        n_batch = len(vision_embedding)\n",
    "        \n",
    "        # normalize dimensions\n",
    "        new_vision_emb = self.dimension_caster(vision_embedding)\n",
    "        \n",
    "        # concatenate\n",
    "        concatenated_emb = torch.cat([new_vision_emb, text_embedding], dim = 1)\n",
    "        \n",
    "        # create attention mask\n",
    "        vision_attention_mask = torch.ones(n_batch, self.n_visual_tokens).to(text_attn_mask.device)\n",
    "        attn_mask = torch.cat([vision_attention_mask, text_attn_mask], dim = 1)\n",
    "        \n",
    "        attn_mask = attn_mask[:, None, None, :]\n",
    "        \n",
    "        # forward\n",
    "        return self.base_model(concatenated_emb, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DIMENSION = 224\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "SAVE_PATH = 'Models/Mamo_S/checkpoint_'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# ViT config\n",
    "feature_extractor = transformers.AutoFeatureExtractor.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "## image transforms\n",
    "img_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.int8, scale = True),\n",
    "    v2.Resize(size = (DIMENSION, DIMENSION), antialias = False),\n",
    "    v2.RandAugment(),\n",
    "    # v2.RandomVerticalFlip(),\n",
    "    # v2.RandomHorizontalFlip(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean = [0.5, 0.5, 0.5],\n",
    "        std =  [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class MaskGenerator:\n",
    "    def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):\n",
    "        self.input_size = input_size\n",
    "        self.mask_patch_size = mask_patch_size\n",
    "        self.model_patch_size = model_patch_size\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "        assert self.input_size % self.mask_patch_size == 0\n",
    "        assert self.mask_patch_size % self.model_patch_size == 0\n",
    "        \n",
    "        self.rand_size = self.input_size // self.mask_patch_size\n",
    "        self.scale = self.mask_patch_size // self.model_patch_size\n",
    "        \n",
    "        self.token_count = self.rand_size ** 2\n",
    "        self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))\n",
    "        \n",
    "    def __call__(self):\n",
    "        mask_idx = np.random.permutation(self.token_count)[:self.mask_count]\n",
    "        mask = np.zeros(self.token_count, dtype=int)\n",
    "        mask[mask_idx] = 1\n",
    "        \n",
    "        mask = mask.reshape((self.rand_size, self.rand_size))\n",
    "        mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "unfold_dim = int(DIMENSION/16)\n",
    "fold_params = {'kernel_size' : unfold_dim,\n",
    "              'dilation': 16}\n",
    "# utils\n",
    "def get_patches(batch, num_patches, size_patch):\n",
    "    '''Function to get patches from an image\n",
    "    \n",
    "    Arguments:\n",
    "    1. batch: batch of tensor images (N, C, H, W)\n",
    "    2. num_patches: number of patches per side\n",
    "    3. size_patch: number of pixels along each side of the patch\n",
    "    \n",
    "    Returns:\n",
    "    Tensor containing patch-wise representation of each image in the batch\n",
    "    '''\n",
    "    return torch.nn.functional.unfold(\n",
    "        batch,\n",
    "        **fold_params\n",
    "        ).reshape(batch.size(0), -1, num_patches**2, size_patch**2).permute(0,2,1,3)\n",
    "\n",
    "def get_img_from_patches(batch, num_patches, size_patch):\n",
    "    '''Function to reconstruct an image from its patches\n",
    "    \n",
    "    Arguments:\n",
    "    1. batch: batch of tensor images (N, C, H, W)\n",
    "    2. num_patches: number of patches per side\n",
    "    3. size_patch: number of pixels along each side of the patch\n",
    "    \n",
    "    Returns:\n",
    "    tensor containing reconstructed images'''\n",
    "    # return fold(batch.permute(0,2,1,3).reshape(batch.size(0), -1, size_patch**2))\n",
    "    return torch.nn.functional.fold(\n",
    "        batch.permute(0,2,1,3).reshape(batch.size(0), -1, size_patch**2),\n",
    "        output_size = (DIMENSION, DIMENSION),\n",
    "        **fold_params\n",
    "    )\n",
    "    \n",
    "class TextMaskGenerator:\n",
    "    def __init__(self, masking_ratio = 0.25, mask_token = '[MASK]'):\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.mask_token = mask_token\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        text = np.array(text.split())  # tokenized\n",
    "        len_txt = len(text)\n",
    "        \n",
    "        n_to_mask = math.ceil(len_txt * self.masking_ratio)\n",
    "        rankings = np.random.randn(len_txt)\n",
    "        \n",
    "        indices = np.argpartition(rankings, -n_to_mask)[-n_to_mask:]\n",
    "        text[indices] = self.mask_token\n",
    "        \n",
    "        \n",
    "        \n",
    "        return \" \".join(text)\n",
    "        \n",
    "\n",
    "class Flickr30K_MAMO(torchvision.datasets.Flickr30k):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 ann_path,\n",
    "                 img_transform = None,\n",
    "                 txt_transform = None,\n",
    "                 max_length = 100,\n",
    "                 ):\n",
    "        super().__init__(data_path, ann_path)\n",
    "        self.img_transform = img_transform\n",
    "        self.tokenizer = txt_transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.img_masker = MaskGenerator(input_size = 224,\n",
    "                                        mask_patch_size = 32,\n",
    "                                        model_patch_size = 16,\n",
    "                                        mask_ratio = MASKING_RATIO_IMG)       #0.75 masking ratio with MAMO\n",
    "        \n",
    "        self.txt_masker = TextMaskGenerator(masking_ratio= MASKING_RATIO_TXT,\n",
    "                                            mask_token = self.tokenizer.mask_token)\n",
    "        \n",
    "        \n",
    "    def process_string(self, string):\n",
    "        tok_str = string.lower().split() # separated by spaces\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        proc_str = [x for x in tok_str if x not in stopwords]               # stopword removal\n",
    "        proc_str = [word.lower() for word in proc_str if word.isalpha()]    # punctuation removal\n",
    "        \n",
    "        return \" \".join(proc_str)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, txt = super().__getitem__(idx)\n",
    "        txt = random.choice(txt)\n",
    "        \n",
    "        \n",
    "        # get images and texts\n",
    "        img = self.img_transform(img)\n",
    "        \n",
    "        # process string\n",
    "        txt = self.process_string(txt)\n",
    "        mask_txt = self.txt_masker(txt)\n",
    "        \n",
    "        tok_text = self.tokenizer(txt, truncation = True, padding = 'max_length', max_length = self.max_length, return_token_type_ids=False)\n",
    "        tok_masked_txt = self.tokenizer(mask_txt, truncation = True, padding = 'max_length', max_length = self.max_length, return_token_type_ids=False)\n",
    "        toks, attn_mask = tok_text['input_ids'], tok_text['attention_mask']\n",
    "        masked_toks, masked_attn_mask = tok_masked_txt['input_ids'], tok_masked_txt['attention_mask']\n",
    "        \n",
    "        toks, attn_mask, masked_toks, masked_attn_mask = torch.tensor(toks), torch.tensor(attn_mask), torch.tensor(masked_toks), torch.tensor(masked_attn_mask)\n",
    "        \n",
    "        # masked indices\n",
    "        mask_indices = (masked_toks == tokenizer.mask_token_id)\n",
    "        \n",
    "        # generate mask for image and text\n",
    "        img_mask = self.img_masker()\n",
    "        \n",
    "        return img, img_mask, toks, attn_mask, masked_toks, masked_attn_mask, mask_indices\n",
    "        \n",
    "\n",
    "def create_masked_image(img, mask):\n",
    "    mask = (mask .repeat_interleave(16, 1)\n",
    "                .repeat_interleave(16, 2)\n",
    "                .unsqueeze(1)\n",
    "                .contiguous()\n",
    "            )\n",
    "    return (img * (1 - mask))\n",
    "      \n",
    "        \n",
    "dataset = Flickr30K_MAMO(DATASET_SRC + 'flickr30k-images',\n",
    "               DATASET_SRC + 'results_20130124.token',\n",
    "               img_transform=img_transform,\n",
    "               txt_transform=tokenizer,\n",
    "               max_length = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    pin_memory = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 30522)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMO(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 vit,\n",
    "                 bert,\n",
    "                 vit_num_patches = 196,\n",
    "                 vit_emb_dim = 384,\n",
    "                 bert_emb_dim = 512,\n",
    "                 bert_layers = 2,\n",
    "                 vocab_size = 30522,\n",
    "                 mask_token_id = 103):\n",
    "       super().__init__()\n",
    "       self.vit = vit\n",
    "       self.bert = bert.base_model\n",
    "       self.bert = deleteEncodingLayers(self.bert.base_model, bert_layers)\n",
    "       self.mamo = MAMO_mixer(bert, bert_layers, 197, vit_emb_dim)\n",
    "       \n",
    "       # vit patches data\n",
    "       self.vit_num_patches = vit_num_patches\n",
    "       \n",
    "       # vocab size\n",
    "       self.vocab_size = vocab_size\n",
    "       # mask token\n",
    "       self.mask_token_id = mask_token_id\n",
    "       \n",
    "       # learnable temperature parameter\n",
    "       self.tau = torch.nn.Parameter(torch.randn(1))\n",
    "       self.tau.requires_grad = True\n",
    "\n",
    "       # joint representation\n",
    "       self.pooler = torch.nn.Sequential(\n",
    "           torch.nn.AdaptiveAvgPool1d(1),\n",
    "           torch.nn.Flatten()\n",
    "       )\n",
    "       self.img_proj = torch.nn.Linear(vit_emb_dim, min(vit_emb_dim, bert_emb_dim))\n",
    "       self.txt_proj = torch.nn.Linear(bert_emb_dim, min(vit_emb_dim, bert_emb_dim))\n",
    "\n",
    "       \n",
    "       # masked representation modeling\n",
    "       self.mrm_proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bert_emb_dim, bert_emb_dim),\n",
    "            torch.nn.Tanh(),\n",
    "       )\n",
    "       \n",
    "       # head for masked image modeling\n",
    "       self.mim_proj = torch.nn.Sequential(\n",
    "           torch.nn.Linear(bert_emb_dim, vit_emb_dim),\n",
    "       )\n",
    "        \n",
    "       # head for masked language modeling\n",
    "       self.mlm_head = bert.cls\n",
    "       \n",
    "       self.itm__head = torch.nn.Sequential(\n",
    "           torch.nn.Linear(bert_emb_dim, bert_emb_dim),\n",
    "           torch.nn.LeakyReLU(),\n",
    "           torch.nn.Linear(bert_emb_dim, 1)\n",
    "       )\n",
    "    \n",
    "       \n",
    "       \n",
    "    def forward(self, image, text, attn_mask,\n",
    "                masked_modeling = True,\n",
    "                masked_image = None,\n",
    "                masked_text = None,\n",
    "                image_text_matching = False\n",
    "                ):\n",
    "        \n",
    "        if image_text_matching == True:\n",
    "            img_rep = self.vit(image)['last_hidden_state']\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']\n",
    "            joint_rep = self.mamo(img_rep, txt_rep, attn_mask)['last_hidden_state'][:, 0, :]\n",
    "            \n",
    "            return self.itm__head(joint_rep)\n",
    "            \n",
    "        \n",
    "        if not masked_modeling:\n",
    "            img_rep = self.vit(image)['last_hidden_state']\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']\n",
    "            joint_rep = self.mamo(img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "        \n",
    "            return (img_rep, txt_rep, joint_rep)\n",
    "        \n",
    "        else:\n",
    "            # return mask_img-clean_txt, clean_img,-mask_txt, \n",
    "            img_rep = self.vit(image)['last_hidden_state']              # clean image\n",
    "            txt_rep = self.bert(text, attn_mask)['last_hidden_state']   # clean text\n",
    "            \n",
    "            mask_img_rep = self.vit(masked_image)['last_hidden_state']\n",
    "            mask_txt_rep = self.bert(masked_text, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            # multimodal prediction\n",
    "            c_img_m_txt = self.mamo(img_rep, mask_txt_rep, attn_mask)['last_hidden_state']\n",
    "            m_img_c_txt = self.mamo(mask_img_rep, txt_rep, attn_mask)['last_hidden_state']\n",
    "            \n",
    "            # pure txt\n",
    "            txt_prediction = self.mlm_head(mask_txt_rep)\n",
    "            \n",
    "            # pool and flatten text and visual features obtained before fusion\n",
    "            img_rep = self.img_proj(self.pooler(img_rep.transpose(1,2)))\n",
    "            txt_rep = self.txt_proj(self.pooler(txt_rep.transpose(1,2)))\n",
    "            \n",
    "            return (c_img_m_txt, m_img_c_txt, mask_img_rep, txt_prediction, img_rep, txt_rep)\n",
    "            \n",
    "    def mrm_projection(self, rep):\n",
    "        return self.mrm_proj(rep)\n",
    "    \n",
    "    def mim_projection(self, rep):\n",
    "        return self.mim_proj(rep)\n",
    "    \n",
    "    def get_mrm_loss(self, online_representation, target_representation, mask):\n",
    "        # remove cls token\n",
    "        on_rep = self.mrm_projection(online_representation[:, 1:, :])\n",
    "        tr_rep = target_representation[:, 1:, :]\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(on_rep, tr_rep, reduction = 'none')\n",
    "        mrm_loss = (loss * mask).sum()/(mask.sum() + 1e-5)              # add for 0 division errors\n",
    "        return mrm_loss\n",
    "    \n",
    "    def get_mim_loss(self, online_representation, target_representation, mask):\n",
    "        on_rep = self.mim_projection(online_representation[:, 1:self.vit_num_patches+1, :]) # omit cls token\n",
    "        tr_rep = target_representation[:, 1:self.vit_num_patches+1, :]\n",
    "        \n",
    "        loss = torch.nn.functional.l1_loss(on_rep, tr_rep, reduction = 'none')\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask[:, :, None]\n",
    "        mim_loss = (loss * mask).sum() / (mask.sum() + 1e-5)\n",
    "        return mim_loss \n",
    "    \n",
    "    \n",
    "    def get_mlm_loss(self, scores, sen, masked_sen):\n",
    "        labels = torch.where(masked_sen == self.mask_token_id, sen, -100)\n",
    "        loss = torch.nn.functional.cross_entropy(scores.view(-1, self.vocab_size), labels.view(-1), ignore_index=-100)\n",
    "        # loss = loss_fct(scores.view(-1, self.vocab_size), labels.view(-1))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def get_itc_loss(self, img_feats, txt_feats):\n",
    "        # Calculate cosine similarity\n",
    "        sim = torch.exp((img_feats@txt_feats.T)/self.tau)\n",
    "        self_mask = torch.eye(sim.shape[0], device=sim.device)\n",
    "\n",
    "        return sim, (torch.nn.functional.cross_entropy(sim, self_mask) + torch.nn.functional.cross_entropy(sim.T, self_mask))/2.\n",
    "    \n",
    "    def get_samples(self, similarities):\n",
    "        probs = torch.nn.functional.softmax(similarities, dim = 1)\n",
    "        txt_indices = torch.multinomial(probs, num_samples=1, replacement=True).squeeze(1)\n",
    "        img_indices = torch.multinomial(probs.T, num_samples=1, replacement=True).squeeze(1)\n",
    "        \n",
    "        return txt_indices, img_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vit_model = transformers.ViTModel.from_pretrained('WinKawaks/vit-small-patch16-224').to(DEVICE)\n",
    "bert_model = transformers.BertForMaskedLM.from_pretrained(\"prajjwal1/bert-small\")\n",
    "\n",
    "online_network = MAMO(\n",
    "                    vit = vit_model,\n",
    "                    bert = bert_model,\n",
    "                    vit_num_patches= 196,\n",
    "                    vit_emb_dim=384,\n",
    "                    bert_emb_dim=512,\n",
    "                    bert_layers=n_layers,\n",
    "                    vocab_size=tokenizer.vocab_size,\n",
    "                    mask_token_id= tokenizer.mask_token_id\n",
    "                ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for target network\n",
    "\n",
    "# freeze weights\n",
    "def freeze_weights(nw):\n",
    "    for param in nw.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return nw\n",
    "    \n",
    "def ewma_weights(target, current, alpha = 0.995):\n",
    "    sdA = target.state_dict()\n",
    "    sdB = current.state_dict()\n",
    "    \n",
    "    for key in sdA:\n",
    "        sdA[key] = alpha*sdA[key] + (1-alpha)*sdB[key]\n",
    "    \n",
    "    target.load_state_dict(sdA)\n",
    "    return target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = copy.deepcopy(online_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08269abd17b04c07ba58e809e460ad48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ML Projects\\Denoising MAMO\\wandb\\run-20240307_022938-ci3xdvn5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain/runs/ci3xdvn5' target=\"_blank\">MAMO - ViT-S, BERT-S</a></strong> to <a href='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain' target=\"_blank\">https://wandb.ai/madhava20217/MAMO%20-%20Pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain/runs/ci3xdvn5' target=\"_blank\">https://wandb.ai/madhava20217/MAMO%20-%20Pretrain/runs/ci3xdvn5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain/runs/ci3xdvn5?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x22184b58810>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimiser\n",
    "optim = torch.optim.AdamW(online_network.parameters(),\n",
    "                          lr = lr,\n",
    "                          weight_decay = decay,\n",
    "                          betas = [0.9, 0.999],\n",
    "                          )\n",
    "\n",
    "epoch_steps = math.ceil(len(dataset)/BATCH_SIZE)\n",
    "num_steps = int(EPOCHS * epoch_steps)\n",
    "warmup_steps = int(warmup_epochs * epoch_steps)\n",
    "\n",
    "lr_scheduler = CosineLRScheduler(\n",
    "        optim,\n",
    "        t_initial=num_steps,\n",
    "        # t_mul=1.,\n",
    "        lr_min=min_lr,\n",
    "        warmup_lr_init = init_lr,\n",
    "        warmup_t=warmup_steps,\n",
    "        cycle_limit=1,\n",
    "        t_in_epochs=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# wandB init\n",
    "wandb.init(\n",
    "    id = id,# id,\n",
    "    resume =  'allow',\n",
    "    project = 'MAMO - Pretrain',\n",
    "    name = 'MAMO - ViT-S, BERT-S',\n",
    "\n",
    "    config = {\n",
    "        'architecture': model_name,\n",
    "        'dataset':'ImageNet1K',\n",
    "        'warmup_epochs': warmup_epochs,\n",
    "        'epochs' : EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'masking_ratio_img' : MASKING_RATIO_IMG,\n",
    "        'masking_ratio_itxt' : MASKING_RATIO_TXT,\n",
    "        'mask_patch_size': 196,\n",
    "        'image_size' : DIMENSION,\n",
    "        'optim_params':{\n",
    "            'optim': 'AdamW',\n",
    "            'beta1': beta1,\n",
    "            'beta2': beta2,\n",
    "            'weight_decay': decay,\n",
    "            'learning_rate': lr,\n",
    "        },\n",
    "        'accumulation_iters': 1,\n",
    "        'patch_size_mask' : 32,\n",
    "        'alpha_ewma': ALPHA,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nums = [int(re.match(r'.*checkpoint_(\\d+).*', x).group(1)) for x in glob.glob(MODEL_SAVE_PATH+'*.pth')]\n",
    "\n",
    "CHKPT = -1\n",
    "\n",
    "if len(nums) != 0:\n",
    "    CHKPT = max(nums)\n",
    "\n",
    "    load_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, CHKPT)\n",
    "    chkpt = torch.load(load_path, map_location = {'cuda:1': device, \n",
    "                                                  'cuda:0': device})\n",
    "\n",
    "    online_network.load_state_dict(chkpt['online_model_state_dict'])\n",
    "    target_network.load_state_dict(chkpt['target_model_state_dict'])\n",
    "    optim.load_state_dict(chkpt['optim_state_dict'])\n",
    "    # lr_scheduler.load_state_dict(chkpt['scheduler_state_dict'])\n",
    "    \n",
    "    print(load_path)\n",
    "    \n",
    "    print(\"loaded earlier settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = freeze_weights(target_network).to(DEVICE)\n",
    "scaler = torch.cuda.amp.grad_scaler.GradScaler()\n",
    "itm_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    num_samples = 0\n",
    "    pretrain_loss = 0\n",
    "    for idx, data in (pbar := tqdm(enumerate(pretrain_dataloader), total = len(pretrain_dataloader))):\n",
    "        img, img_mask, txt, attn_mask, masked_toks, masked_attn_mask, mask_indices = data\n",
    "        \n",
    "        # vision\n",
    "        img = img.to(DEVICE)\n",
    "        img_mask = img_mask.to(DEVICE)\n",
    "        \n",
    "        # language\n",
    "        txt = txt.to(DEVICE)\n",
    "        attn_mask = attn_mask.to(DEVICE)\n",
    "        masked_toks = masked_toks.to(DEVICE)\n",
    "        masked_attn_mask = masked_attn_mask.to(DEVICE)\n",
    "\n",
    "        # indices for masked text: will be used for masked modeling\n",
    "        mask_indices = mask_indices.float().to(DEVICE)\n",
    "        \n",
    "        # masked image\n",
    "        masked_image = create_masked_image(img, img_mask)\n",
    "        flattened_img_mask = img_mask.float().flatten(1)\n",
    "        \n",
    "        # create masks for joint representation modeling\n",
    "        img_rep_masks = torch.cat([flattened_img_mask, torch.zeros_like(mask_indices)], axis = 1).unsqueeze(-1)\n",
    "        txt_rep_masks = torch.cat([torch.zeros_like(flattened_img_mask), mask_indices], axis = 1).unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # masked modeling pretraining\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):                           # casting to f16\n",
    "            \n",
    "            \n",
    "            # forward step for target network\n",
    "            with torch.no_grad():\n",
    "                target_img_rep, target_txt_rep, target_mm_rep = target_network(img, txt, attn_mask, masked_modeling = False)\n",
    "            \n",
    "            # forward step for online network\n",
    "            c_img_m_txt, m_img_c_txt, mask_img_rep, txt_prediction, img_rep, txt_rep = online_network(img, txt, attn_mask,\n",
    "                                                                                        masked_modeling = True,\n",
    "                                                                                        masked_image = masked_image,\n",
    "                                                                                        masked_text = masked_toks)\n",
    "\n",
    "\n",
    "            # MRM loss\n",
    "            mrm_loss_txt = online_network.get_mrm_loss(c_img_m_txt, target_mm_rep, txt_rep_masks)\n",
    "            mrm_loss_img = online_network.get_mrm_loss(m_img_c_txt, target_mm_rep, img_rep_masks)\n",
    "            \n",
    "            # MIM loss\n",
    "            mim_loss = online_network.get_mim_loss(m_img_c_txt, target_img_rep, flattened_img_mask)\n",
    "            \n",
    "            # MLM loss\n",
    "            mlm_loss = online_network.get_mlm_loss(txt_prediction, txt, masked_toks)\n",
    "            \n",
    "            \n",
    "            # ITC loss\n",
    "            sim, itc_loss = online_network.get_itc_loss(img_rep, txt_rep)\n",
    "            \n",
    "            #itm loss\n",
    "            # sample for each image and each text separately\n",
    "            img_maps, txt_maps = online_network.get_samples(sim)\n",
    "            right_samples = torch.arange(0, img_maps.size(0)).to(DEVICE)\n",
    "            \n",
    "            labs_img = (img_maps == right_samples).float().unsqueeze(1)\n",
    "            labs_txt = (txt_maps == right_samples).float().unsqueeze(1)\n",
    "            \n",
    "            outs_img = online_network(img, txt[txt_maps], attn_mask[txt_maps], image_text_matching = True)\n",
    "            outs_txt = online_network(img[img_maps], txt, attn_mask, image_text_matching = True)\n",
    "            \n",
    "            # softmax probabilities\n",
    "            itm_1 = itm_loss_fn(outs_img, labs_img)\n",
    "            itm_2 = itm_loss_fn(outs_txt, labs_txt)\n",
    "            itm = itm_1 + itm_2\n",
    "            \n",
    "            # TOTAL LOSS\n",
    "            net_loss = (mrm_loss_img + mrm_loss_txt) + (mim_loss) + (mlm_loss) + (itc_loss) + (itm)\n",
    "            \n",
    "        scaler.scale(net_loss).backward()\n",
    "            \n",
    "        # BACKPROP\n",
    "        scaler.step(optim)        # fp16\n",
    "        scaler.update()           # fp16\n",
    "        optim.zero_grad(set_to_none = True)\n",
    "        lr_scheduler.step_update(epoch * epoch_steps + idx)\n",
    "        \n",
    "        # update and calc loss\n",
    "        num_samples+=1\n",
    "        pretrain_loss+= net_loss.item()\n",
    "        pbar.set_description(f\"Train Loss: {pretrain_loss/num_samples}\")\n",
    "        \n",
    "        \n",
    "        # EWMA for weights\n",
    "        target_network = ewma_weights(target_network, online_network, alpha = ALPHA)\n",
    "        \n",
    "        \n",
    "    online_network.load_state_dict(chkpt['online_model_state_dict'])\n",
    "    target_network.load_state_dict(chkpt['target_model_state_dict'])\n",
    "    save_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, epoch)\n",
    "\n",
    "    wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'pretrain_loss': pretrain_loss/num_samples\n",
    "    })\n",
    "    \n",
    "    torch.save(\n",
    "            {\n",
    "            'epoch': epoch,\n",
    "            'online_model_state_dict': online_network.state_dict(),\n",
    "            'target_model_state_dict': target_network.state_dict(),\n",
    "            'optim_state_dict': optim.state_dict()\n",
    "            # 'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            },\n",
    "        save_path\n",
    "        )\n",
    "    if (epoch-warmup_epochs) % 10 == 0:\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1429.7030, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
