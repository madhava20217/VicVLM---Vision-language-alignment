{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ml/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tokenizers\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "\n",
    "import wandb \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from utils.MAMO import MAMO\n",
    "\n",
    "from utils.dataset import pretrain_dataset\n",
    "from utils.mim_utils import create_masked_image\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "\n",
    "DEVICE = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_name = 'vit_bert_s - normalized'\n",
    "algo = 'MAMO'\n",
    "\n",
    "\n",
    "NUM_WORKERS = 12\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "\n",
    "id = wandb.util.generate_id()\n",
    "wandb.login()\n",
    "\n",
    "# set earlier ID\n",
    "id = 'eyf883nb'\n",
    "\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_JSON = 'Jsons/flickr30k_train.json'\n",
    "MODEL_SAVE_PATH = f'Models/{model_name}/{algo}/checkpoint'\n",
    "\n",
    "lr = 2.5e-4\n",
    "init_lr = 1e-6\n",
    "min_lr = 1e-5\n",
    "decay = 0.01\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "warmup_epochs = 5\n",
    "EPOCHS = 30\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "MASKING_RATIO_IMG = 0.75\n",
    "MASKING_RATIO_TXT = 0.25\n",
    "\n",
    "ALPHA = 0.995               # EWMA\n",
    "\n",
    "\n",
    "n_layers = 3\n",
    "\n",
    "if os.path.exists(os.path.dirname(MODEL_SAVE_PATH)) == False:\n",
    "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 224\n",
    "\n",
    "MAX_LEN = 30\n",
    "\n",
    "# ViT config\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "## image transforms\n",
    "img_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.int8, scale = True),\n",
    "    v2.Resize(size = (DIMENSION, DIMENSION), antialias = False),\n",
    "    v2.RandAugment(),\n",
    "    # v2.RandomVerticalFlip(),\n",
    "    # v2.RandomHorizontalFlip(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean = [0.5, 0.5, 0.5],\n",
    "        std =  [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pretrain_dataset(\n",
    "               ann_file = [DATASET_JSON],\n",
    "               transform = img_transform,\n",
    "               tokenizer = tokenizer,\n",
    "               max_words = MAX_LEN,\n",
    "               input_size = DIMENSION,\n",
    "               mask_patch_size = 32,\n",
    "               model_patch_size = 16,\n",
    "               masking_ratio = MASKING_RATIO_IMG,\n",
    "               txt_masking_ratio = MASKING_RATIO_TXT,\n",
    "               mask_token = tokenizer.mask_token,\n",
    "               mask_token_id = tokenizer.mask_token_id,\n",
    "               max_length = MAX_LEN + 5,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    pin_memory = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vit_model = transformers.ViTModel.from_pretrained('WinKawaks/vit-small-patch16-224').to(DEVICE)\n",
    "bert_model = transformers.BertForMaskedLM.from_pretrained(\"prajjwal1/bert-small\")\n",
    "\n",
    "online_network = MAMO(\n",
    "                    vit = vit_model,\n",
    "                    bert = bert_model,\n",
    "                    vit_num_patches= 196,\n",
    "                    vit_emb_dim=384,\n",
    "                    bert_emb_dim=512,\n",
    "                    bert_layers=n_layers,\n",
    "                    vocab_size=tokenizer.vocab_size,\n",
    "                    mask_token_id= tokenizer.mask_token_id,\n",
    "                    # cls_token_id=tokenizer.cls_token_id\n",
    "                ).train().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for target network\n",
    "\n",
    "# freeze weights\n",
    "def freeze_weights(nw):\n",
    "    for param in nw.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return nw\n",
    "    \n",
    "def ewma_weights(target, current, alpha = 0.995):\n",
    "    sdA = target.state_dict()\n",
    "    sdB = current.state_dict()\n",
    "    \n",
    "    for key in sdA:\n",
    "        sdA[key] = alpha*sdA[key] + (1-alpha)*sdB[key]\n",
    "    \n",
    "    target.load_state_dict(sdA)\n",
    "    return target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = copy.deepcopy(online_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ml/ML Projects/Denoising MAMO/wandb/run-20240309_202256-eyf883nb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain/runs/eyf883nb' target=\"_blank\">MAMO - ViT-S, BERT-S</a></strong> to <a href='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain' target=\"_blank\">https://wandb.ai/madhava20217/MAMO%20-%20Pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain/runs/eyf883nb' target=\"_blank\">https://wandb.ai/madhava20217/MAMO%20-%20Pretrain/runs/eyf883nb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/madhava20217/MAMO%20-%20Pretrain/runs/eyf883nb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5c82b05090>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimiser\n",
    "optim = torch.optim.AdamW(online_network.parameters(),\n",
    "                          lr = lr,\n",
    "                          weight_decay = decay,\n",
    "                          betas = [0.9, 0.999],\n",
    "                          )\n",
    "\n",
    "epoch_steps = math.ceil(len(dataset)/BATCH_SIZE)\n",
    "num_steps = int(EPOCHS * epoch_steps)\n",
    "warmup_steps = int(warmup_epochs * epoch_steps)\n",
    "\n",
    "lr_scheduler = CosineLRScheduler(\n",
    "        optim,\n",
    "        t_initial=num_steps,\n",
    "        # t_mul=1.,\n",
    "        lr_min=min_lr,\n",
    "        warmup_lr_init = init_lr,\n",
    "        warmup_t=warmup_steps,\n",
    "        cycle_limit=1,\n",
    "        t_in_epochs=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# wandB init\n",
    "wandb.init(\n",
    "    id = id,# id,\n",
    "    resume =  'allow',\n",
    "    project = 'MAMO - Pretrain',\n",
    "    name = 'MAMO - ViT-S, BERT-S',\n",
    "\n",
    "    config = {\n",
    "        'architecture': model_name,\n",
    "        'dataset':'ImageNet1K',\n",
    "        'warmup_epochs': warmup_epochs,\n",
    "        'epochs' : EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'masking_ratio_img' : MASKING_RATIO_IMG,\n",
    "        'masking_ratio_itxt' : MASKING_RATIO_TXT,\n",
    "        'mask_patch_size': 196,\n",
    "        'image_size' : DIMENSION,\n",
    "        'optim_params':{\n",
    "            'optim': 'AdamW',\n",
    "            'beta1': beta1,\n",
    "            'beta2': beta2,\n",
    "            'weight_decay': decay,\n",
    "            'learning_rate': lr,\n",
    "        },\n",
    "        'accumulation_iters': 1,\n",
    "        'patch_size_mask' : 32,\n",
    "        'alpha_ewma': ALPHA,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nums = [re.match(r'.*checkpoint_(.*).pth', x).group(1) for x in glob.glob(MODEL_SAVE_PATH+'*.pth')]\n",
    "if len(nums) > 0:\n",
    "    nums.remove(\"final\")\n",
    "nums = [int(x) for x in nums]\n",
    "\n",
    "CHKPT = -1\n",
    "\n",
    "if len(nums) != 0:\n",
    "    CHKPT = max(nums)\n",
    "\n",
    "    load_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, CHKPT)\n",
    "    chkpt = torch.load(load_path, map_location = {'cuda:1': device, \n",
    "                                                  'cuda:0': device})\n",
    "\n",
    "    online_network.load_state_dict(chkpt['online_model_state_dict'])\n",
    "    target_network.load_state_dict(chkpt['target_model_state_dict'])\n",
    "    optim.load_state_dict(chkpt['optim_state_dict'])\n",
    "    # lr_scheduler.load_state_dict(chkpt['scheduler_state_dict'])\n",
    "    \n",
    "    print(load_path)\n",
    "    \n",
    "    print(\"loaded earlier settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2266 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 179.69800186157227:   2%|â–         | 40/2266 [00:28<23:16,  1.59it/s] "
     ]
    }
   ],
   "source": [
    "target_network = freeze_weights(target_network).to(DEVICE).eval()\n",
    "scaler = torch.cuda.amp.grad_scaler.GradScaler()\n",
    "itm_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(CHKPT+1, EPOCHS + warmup_epochs):\n",
    "    num_samples = 0\n",
    "    pretrain_loss = 0\n",
    "    # net all losses\n",
    "    net_mrm_loss = 0\n",
    "    net_mim_loss = 0\n",
    "    net_mlm_loss = 0\n",
    "    net_itc_loss = 0\n",
    "    net_itm_loss = 0\n",
    "    for idx, data in (pbar := tqdm(enumerate(pretrain_dataloader), total = len(pretrain_dataloader))):\n",
    "        img, img_mask, txt, attn_mask, masked_toks, masked_attn_mask, mask_indices = data\n",
    "        \n",
    "        # vision\n",
    "        img = img.to(DEVICE)\n",
    "        img_mask = img_mask.to(DEVICE)\n",
    "        \n",
    "        # language\n",
    "        txt = txt.to(DEVICE)\n",
    "        attn_mask = attn_mask.to(DEVICE)\n",
    "        masked_toks = masked_toks.to(DEVICE)\n",
    "        masked_attn_mask = masked_attn_mask.to(DEVICE)\n",
    "\n",
    "        # indices for masked text: will be used for masked modeling\n",
    "        mask_indices = mask_indices.float().to(DEVICE)\n",
    "        \n",
    "        # masked image\n",
    "        masked_image = create_masked_image(img, img_mask)\n",
    "        flattened_img_mask = img_mask.float().flatten(1)\n",
    "        \n",
    "        # create masks for joint representation modeling\n",
    "        img_rep_masks = torch.cat([flattened_img_mask, torch.zeros_like(mask_indices)], axis = 1).unsqueeze(-1)\n",
    "        txt_rep_masks = torch.cat([torch.zeros_like(flattened_img_mask), mask_indices], axis = 1).unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # masked modeling pretraining\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):                           # casting to f16\n",
    "            \n",
    "            \n",
    "            # forward step for target network\n",
    "            with torch.no_grad():\n",
    "                target_img_rep, target_txt_rep, target_mm_rep, target_itm = target_network(img,\n",
    "                                                                                           txt,\n",
    "                                                                                           attn_mask,\n",
    "                                                                                           image_text_matching = True)\n",
    "            \n",
    "            # forward step for online network\n",
    "            c_img_m_txt, m_img_c_txt, mask_img_rep, txt_prediction, img_rep, txt_rep = online_network(img,\n",
    "                                                                                                      txt,\n",
    "                                                                                                      attn_mask,\n",
    "                                                                                                      image_text_matching = False,\n",
    "                                                                                                      masked_image = masked_image,\n",
    "                                                                                                      masked_text = masked_toks)\n",
    "\n",
    "\n",
    "            # MRM loss\n",
    "            mrm_loss_txt = online_network.get_mrm_loss(c_img_m_txt, target_mm_rep, txt_rep_masks)\n",
    "            mrm_loss_img = online_network.get_mrm_loss(m_img_c_txt, target_mm_rep, img_rep_masks)\n",
    "            \n",
    "            # MIM loss\n",
    "            mim_loss = online_network.get_mim_loss(m_img_c_txt, target_img_rep, flattened_img_mask)\n",
    "            \n",
    "            # MLM loss\n",
    "            mlm_loss = online_network.get_mlm_loss(txt_prediction, txt, masked_toks)\n",
    "            \n",
    "            \n",
    "            # ITC loss\n",
    "            sim, itc_loss = online_network.get_itc_loss(img_rep, txt_rep)\n",
    "            \n",
    "            #itm loss\n",
    "            # sample for each image and each text separately\n",
    "            img_maps, txt_maps = online_network.get_samples(sim)\n",
    "            right_samples = torch.arange(0, img_maps.size(0)).to(DEVICE)\n",
    "            \n",
    "            labs_img = (img_maps == right_samples).float().unsqueeze(1)\n",
    "            labs_txt = (txt_maps == right_samples).float().unsqueeze(1)\n",
    "            \n",
    "            outs_img = online_network(img, txt[txt_maps], attn_mask[txt_maps], image_text_matching = True)[-1]\n",
    "            outs_txt = online_network(img[img_maps], txt, attn_mask, image_text_matching = True)[-1]\n",
    "            \n",
    "            # softmax probabilities\n",
    "            itm_1 = itm_loss_fn(outs_img, labs_img)\n",
    "            itm_2 = itm_loss_fn(outs_txt, labs_txt)\n",
    "            itm_loss = itm_1 + itm_2\n",
    "            \n",
    "            # TOTAL LOSS\n",
    "            net_loss = (mrm_loss_img + mrm_loss_txt) + (mim_loss) + (mlm_loss) + (itc_loss) + (itm_loss)\n",
    "            \n",
    "        scaler.scale(net_loss).backward()\n",
    "            \n",
    "        # BACKPROP\n",
    "        scaler.step(optim)        # fp16\n",
    "        scaler.update()           # fp16\n",
    "        optim.zero_grad(set_to_none = True)\n",
    "        lr_scheduler.step_update(epoch * epoch_steps + idx)\n",
    "        \n",
    "        # update and calc loss\n",
    "        num_samples+=1\n",
    "        net_mrm_loss+= mrm_loss_img.item() + mrm_loss_txt.item()\n",
    "        net_mim_loss+= mim_loss.item()\n",
    "        net_mlm_loss+= mlm_loss.item()\n",
    "        net_itc_loss+= itc_loss.item()\n",
    "        net_itm_loss+= itm_loss.item()\n",
    "        pretrain_loss+= net_loss.item()\n",
    "        pbar.set_description(f\"Train Loss: {pretrain_loss/num_samples}\")\n",
    "        \n",
    "        \n",
    "        # EWMA for weights\n",
    "        target_network = ewma_weights(target_network, online_network, alpha = ALPHA)\n",
    "        \n",
    "        \n",
    "\n",
    "    save_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, epoch)\n",
    "\n",
    "    wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'pretrain_loss': pretrain_loss/num_samples,\n",
    "            'mrm_loss': net_mrm_loss/num_samples,\n",
    "            'mim_loss': net_mim_loss/num_samples,\n",
    "            'mlm_loss': net_mlm_loss/num_samples,\n",
    "            'itc_loss': net_itc_loss/num_samples,\n",
    "            'itm_loss': net_itm_loss/num_samples,\n",
    "    })\n",
    "    \n",
    "    torch.save(\n",
    "            {\n",
    "            'epoch': epoch,\n",
    "            'online_model_state_dict': online_network.state_dict(),\n",
    "            'target_model_state_dict': target_network.state_dict(),\n",
    "            'optim_state_dict': optim.state_dict()\n",
    "            },\n",
    "        save_path\n",
    "        )\n",
    "    if (epoch-warmup_epochs+1) % 25 == 0:\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}_{}.pth'.format(MODEL_SAVE_PATH, 'final')\n",
    "torch.save(online_network.state_dict(), save_path)\n",
    "wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
